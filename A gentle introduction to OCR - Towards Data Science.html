<!DOCTYPE html>
<!-- saved from url=(0072)https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./A gentle introduction to OCR - Towards Data Science_files/branch-latest.min.js"></script><script async="" src="./A gentle introduction to OCR - Towards Data Science_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>A gentle introduction to OCR - Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-06-25T08:55:17.177Z"><meta data-rh="true" name="title" content="A gentle introduction to OCR - Towards Data Science"><meta data-rh="true" property="og:title" content="A gentle introduction to OCR"><meta data-rh="true" property="twitter:title" content="A gentle introduction to OCR"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/ee1469a201aa"><meta data-rh="true" property="al:android:url" content="medium://p/ee1469a201aa"><meta data-rh="true" property="al:ios:url" content="medium://p/ee1469a201aa"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="OCR, or optical character recognition, is one of the earliest addressed computer vision tasks, since in some aspects it does not require deep learning. Therefore there were different OCR…"><meta data-rh="true" property="og:description" content="How and why to apply deep learning to Optical Character Recognition"><meta data-rh="true" property="twitter:description" content="How and why to apply deep learning to Optical Character Recognition"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/448/1*77Hn7O_KROhi37Gt7Aq67Q.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/448/1*77Hn7O_KROhi37Gt7Aq67Q.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@gidishperber"><meta data-rh="true" name="twitter:creator" content="@shgidi"><meta data-rh="true" name="author" content="Gidi Shperber"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="16 min read"><meta data-rh="true" name="parsely-post-id" content="ee1469a201aa"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./A gentle introduction to OCR - Towards Data Science_files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@gidishperber"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/ee1469a201aa"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="./A gentle introduction to OCR - Towards Data Science_files/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="491" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:fixed}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ai{max-width:1192px}.aj{min-width:0}.ak{width:100%}.al{height:65px}.ao{flex:1 0 auto}.ap{flex:0 0 auto}.aq{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ar{font-style:normal}.as{line-height:20px}.at{font-size:15.8px}.au{letter-spacing:0px}.av{color:rgba(0, 0, 0, 0.54)}.aw{fill:rgba(0, 0, 0, 0.54)}.ax{justify-content:flex-end}.ay{margin-top:16px}.az{margin-bottom:16px}.ba{display:inherit}.bb{max-width:210px}.bc{text-overflow:ellipsis}.bd{overflow:hidden}.be{white-space:nowrap}.bf{display:inline-block}.bg{border:none}.bh{outline:none}.bi{font:inherit}.bj{font-size:16px}.bk{opacity:0}.bl{position:relative}.bm{width:0px}.bn{transition:width 140ms ease-in}.bo{color:inherit}.bp{fill:inherit}.bq{font-size:inherit}.br{border:inherit}.bs{font-family:inherit}.bt{letter-spacing:inherit}.bu{font-weight:inherit}.bv{padding:0}.bw{margin:0}.bx:hover{cursor:pointer}.by:hover{color:rgba(0, 0, 0, 0.9)}.bz:hover{fill:rgba(0, 0, 0, 0.9)}.ca:focus{outline:none}.cb:disabled{cursor:default}.cc:disabled{color:rgba(0, 0, 0, 0.54)}.cd:disabled{fill:rgba(0, 0, 0, 0.54)}.ce{margin-left:16px}.cf{margin-right:10px}.ci{display:none}.ck{margin-right:16px}.cl{visibility:hidden}.cm{padding:4px 12px}.cn{color:rgba(0, 0, 0, 0.84)}.co{background:0}.cp{border-color:rgba(0, 0, 0, 0.54)}.cq:hover{color:rgba(0, 0, 0, 0.97)}.cr:hover{fill:rgba(0, 0, 0, 0.97)}.cs:hover{border-color:rgba(0, 0, 0, 0.84)}.ct:disabled{fill:rgba(0, 0, 0, 0.76)}.cu:disabled{border-color:rgba(0, 0, 0, 0.2)}.cv:disabled{cursor:inherit}.cw:disabled:hover{color:rgba(0, 0, 0, 0.54)}.cx:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.cy:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.cz{border-radius:4px}.da{border-width:1px}.db{border-style:solid}.dc{box-sizing:border-box}.dd{text-decoration:none}.de{padding-bottom:10px}.df{padding-top:10px}.dg{border-radius:50%}.dh{height:32px}.di{width:32px}.dj{border-top:none}.dk{background-color:rgba(53, 88, 118, 1)}.dm{height:54px}.dn{margin-right:40px}.do{height:36px}.dp{width:100px}.dq{overflow:auto}.dr{flex:0 1 auto}.ds{list-style-type:none}.dt{line-height:40px}.du{overflow-x:auto}.dv{align-items:flex-start}.dw{margin-top:20px}.dx{padding-top:20px}.dy{height:80px}.dz{height:20px}.ea{margin-right:15px}.eb{margin-left:15px}.ec:first-child{margin-left:0}.ed{min-width:1px}.ee{background-color:rgba(197, 210, 225, 1)}.ef{font-weight:300}.eg{font-size:15px}.eh{color:rgba(197, 210, 225, 1)}.ei{text-transform:uppercase}.ej{letter-spacing:1px}.ek:hover{color:rgba(251, 255, 255, 1)}.el:hover{fill:rgba(233, 241, 250, 1)}.em:disabled{color:rgba(150, 171, 191, 1)}.en:disabled{fill:rgba(150, 171, 191, 1)}.eo{margin-bottom:0px}.ep{height:119px}.es{padding-left:24px}.et{padding-right:24px}.eu{margin-left:auto}.ev{margin-right:auto}.ew{max-width:728px}.ex{flex-direction:column}.ey{position:absolute}.ez{top:calc(100vh + 100px)}.fa{bottom:calc(100vh + 100px)}.fb{width:10px}.fc{pointer-events:none}.fd{word-break:break-word}.fe{word-wrap:break-word}.ff:after{display:block}.fg:after{content:""}.fh:after{clear:both}.fi{max-width:680px}.fj{line-height:1.23}.fk{letter-spacing:0}.fl{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.fw{margin-bottom:-0.27em}.gc{margin-top:32px}.gd{justify-content:space-between}.gh{height:48px}.gi{width:48px}.gj{margin-left:12px}.gk{margin-bottom:2px}.gm{max-height:20px}.gn{display:-webkit-box}.go{-webkit-line-clamp:1}.gp{-webkit-box-orient:vertical}.gq:hover{text-decoration:underline}.gr{margin-left:8px}.gs{padding:0px 8px}.gt{color:rgba(90, 118, 144, 1)}.gu{fill:rgba(102, 138, 170, 1)}.gv{border-color:rgba(102, 138, 170, 1)}.gw:hover{color:rgba(84, 108, 131, 1)}.gx:hover{fill:rgba(90, 118, 144, 1)}.gy:hover{border-color:rgba(90, 118, 144, 1)}.gz{line-height:18px}.ha{align-items:flex-end}.hi{padding-right:8px}.hj{margin-right:-6px}.hk{fill:rgba(0, 0, 0, 0.76)}.hl{max-width:448px}.hr{clear:both}.hs{transition:opacity 100ms 400ms}.ht{height:100%}.hu{will-change:transform}.hv{transform:translateZ(0)}.hw{margin:auto}.hx{background-color:rgba(0, 0, 0, 0.05)}.hy{padding-bottom:80.80357142857143%}.hz{filter:blur(20px)}.ia{transform:scale(1.1)}.ib{visibility:visible}.ic{background:rgba(255, 255, 255, 1)}.id{padding-left:30px}.ie{line-height:1.48}.if{letter-spacing:-0.014em}.ig{color:rgba(0, 0, 0, 0.76)}.ih{font-size:24px}.in{margin-bottom:-0.46em}.io{font-size:30px}.ip{line-height:44px}.iq{background-repeat:repeat-x}.ir{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.is{background-size:1px 1px}.it{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.iu{line-height:1.12}.iv{letter-spacing:-0.022em}.iw{font-weight:600}.jf{margin-bottom:-0.28em}.jg{line-height:1.58}.jh{letter-spacing:-0.004em}.ji{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.jt{font-weight:700}.jz{line-height:1.18}.kk{margin-bottom:-0.31em}.kl{padding-bottom:46.470588235294116%}.km{line-height:1.4}.kn{margin-top:10px}.ko{text-align:center}.kr{list-style-type:disc}.ks{margin-left:30px}.kt{padding-left:0px}.kz{max-width:1091px}.la{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.lb{cursor:zoom-in}.lc{z-index:auto}.ld{padding-bottom:66.72777268560954%}.le{max-width:782px}.lf{padding-bottom:32.35294117647059%}.lg{list-style-type:decimal}.lh{max-width:472px}.li{padding-bottom:53.601694915254235%}.lj{max-width:526px}.lk{padding-bottom:70.15209125475285%}.ll{max-width:633px}.lm{padding-bottom:83.72827804107425%}.ln{max-width:468px}.lo{padding-bottom:70.08547008547009%}.lp{max-width:534px}.lq{padding-bottom:9.176029962546817%}.lr{max-width:299px}.ls{padding-bottom:84.2809364548495%}.lt{max-width:280px}.lu{padding-bottom:84.28571428571429%}.lv{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.lw{font-size:28px}.lx{margin-top:30px}.ly:before{content:"..."}.lz:before{letter-spacing:0.6em}.ma:before{text-indent:0.6em}.mb:before{font-style:italic}.mc:before{line-height:1.4}.md{max-width:363px}.me{padding-bottom:105.50964187327824%}.mf{max-width:458px}.mg{padding-bottom:38.646288209606986%}.mh{max-width:341px}.mi{padding-bottom:71.5542521994135%}.mj{max-width:447px}.mk{padding-bottom:117.22595078299776%}.ml{padding-bottom:30.562659846547312%}.mm{font-style:italic}.mn{padding:20px}.mo{background:rgba(0, 0, 0, 0.05)}.mp{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.mq{margin-top:-0.09em}.mr{margin-bottom:-0.09em}.ms{white-space:pre-wrap}.my{max-width:297px}.mz{padding-bottom:46.8013468013468%}.na{max-width:283px}.nb{padding-bottom:100%}.nc{max-width:961px}.nd{padding-bottom:25.18210197710718%}.ne{will-change:opacity}.nf{width:188px}.ng{left:50%}.nh{transform:translateX(406px)}.ni{top:calc(65px + 54px + 14px)}.nl{top:calc(65px + 54px + 40px)}.nn{width:131px}.no{padding-bottom:28px}.np{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.nq{font-size:18px}.nr{padding-bottom:20px}.ns{padding-top:2px}.nt{max-height:120px}.nu{-webkit-line-clamp:6}.nv{padding-top:28px}.nw{margin-bottom:19px}.nx{margin-left:-5px}.ny{margin-right:5px}.nz{outline:0}.oa{border:0}.ob{user-select:none}.oc{cursor:pointer}.od> svg{pointer-events:none}.oe:active{border-style:none}.of{-webkit-user-select:none}.og:focus{fill:rgba(90, 118, 144, 1)}.oh{margin-top:5px}.oi button{text-align:left}.oj{margin-top:40px}.ok{flex-wrap:wrap}.ol{margin-top:25px}.om{margin-right:8px}.on{margin-bottom:8px}.oo{border-radius:3px}.op{padding:5px 10px}.oq{line-height:22px}.or{margin-top:15px}.os{border:1px solid rgba(0, 0, 0, 0.1)}.ot{height:60px}.ou{width:60px}.ph:active{border-style:solid}.pi{z-index:2}.pk{padding-top:32px}.pl{border-top:1px solid rgba(0, 0, 0, 0.1)}.pm{margin-bottom:25px}.pn{margin-bottom:32px}.po{min-height:80px}.pt{width:80px}.pu{padding-left:102px}.pw{letter-spacing:0.05em}.px{margin-bottom:6px}.py{line-height:36px}.pz{max-width:555px}.qa{max-width:450px}.qb{line-height:24px}.qd{max-width:550px}.qe{padding-top:25px}.qf{opacity:1}.qg{border:1px solid rgba(102, 138, 170, 1)}.qh{margin-top:64px}.qi{background-color:rgba(0, 0, 0, 0.02)}.qj{padding:60px 0}.qk{background-color:rgba(0, 0, 0, 0.9)}.rb{padding-bottom:48px}.rc{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.rd{margin:0 -12px}.re{margin:0 12px}.rf{flex:1 1 0}.rg{padding-bottom:12px}.rh:hover{color:rgba(255, 255, 255, 0.99)}.ri:hover{fill:rgba(255, 255, 255, 0.99)}.rj:disabled{color:rgba(255, 255, 255, 0.7)}.rk:disabled{fill:rgba(255, 255, 255, 0.7)}.rl{color:rgba(255, 255, 255, 0.98)}.rm{fill:rgba(255, 255, 255, 0.98)}.rn{text-align:inherit}.ro{font-size:21.6px}.rp{letter-spacing:-0.32px}.rq{color:rgba(255, 255, 255, 0.7)}.rr{fill:rgba(255, 255, 255, 0.7)}.rs{text-decoration:underline}.rt{padding-bottom:8px}.ru{padding-top:8px}.rv{width:200px}.rx{margin:15px 0}.ry{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ah{margin:0 64px}.fu{font-size:40px}.fv{margin-top:0.78em}.gb{line-height:48px}.hh{margin-left:30px}.hq{margin-top:56px}.im{margin-top:2.75em}.jd{font-size:34px}.je{margin-top:1.95em}.jr{font-size:21px}.js{margin-top:0.86em}.jy{margin-top:2em}.ki{font-size:26px}.kj{margin-top:1.72em}.ky{margin-top:1.05em}.mx{margin-top:1.91em}.qy{padding-left:64px}.qz{padding-right:64px}.ra{max-width:1320px}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.hg{margin-left:30px}.kp{margin-left:auto}.kq{text-align:center}.qv{padding-left:64px}.qw{padding-right:64px}.qx{max-width:1080px}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.cj{display:flex}.hf{margin-left:30px}.qs{padding-left:48px}.qt{padding-right:48px}.qu{max-width:904px}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.am{height:56px}.an{display:flex}.cg{margin-left:10px}.ch{margin-right:10px}.dl{display:block}.eq{margin-bottom:0px}.er{height:110px}.gf{margin-top:32px}.gg{flex-direction:column-reverse}.hd{margin-bottom:30px}.he{margin-left:0px}.pp{margin-bottom:24px}.pq{align-items:center}.pr{width:102px}.ps{position:relative}.pv{padding-left:0}.qc{margin-top:24px}.ql{padding:32px 0}.qp{padding-left:24px}.qq{padding-right:24px}.qr{max-width:728px}.rw{width:140px}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ac{margin:0 24px}.fm{font-size:30px}.fn{margin-top:0.72em}.fx{line-height:40px}.ge{margin-top:32px}.gl{margin-bottom:0px}.hb{margin-bottom:30px}.hc{margin-left:0px}.hm{margin-top:40px}.ii{margin-top:1.42em}.ix{margin-top:1.2em}.jj{font-size:18px}.jk{margin-top:0.67em}.ju{margin-top:1.56em}.ka{font-size:24px}.kb{margin-top:1.23em}.ku{margin-top:1.34em}.mt{margin-top:1.41em}.qm{padding-left:24px}.qn{padding-right:24px}.qo{max-width:552px}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ag{margin:0 64px}.fs{font-size:40px}.ft{margin-top:0.78em}.ga{line-height:48px}.hp{margin-top:56px}.il{margin-top:2.75em}.jb{font-size:34px}.jc{margin-top:1.95em}.jp{font-size:21px}.jq{margin-top:0.86em}.jx{margin-top:2em}.kg{font-size:26px}.kh{margin-top:1.72em}.kx{margin-top:1.05em}.mw{margin-top:1.91em}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.af{margin:0 48px}.fq{font-size:40px}.fr{margin-top:0.78em}.fz{line-height:48px}.ho{margin-top:56px}.ik{margin-top:2.75em}.iz{font-size:34px}.ja{margin-top:1.95em}.jn{font-size:21px}.jo{margin-top:0.86em}.jw{margin-top:2em}.ke{font-size:26px}.kf{margin-top:1.72em}.kw{margin-top:1.05em}.mv{margin-top:1.91em}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ae{margin:0 24px}.fo{font-size:30px}.fp{margin-top:0.72em}.fy{line-height:40px}.hn{margin-top:40px}.ij{margin-top:1.42em}.iy{margin-top:1.2em}.jl{font-size:18px}.jm{margin-top:0.67em}.jv{margin-top:1.56em}.kc{font-size:24px}.kd{margin-top:1.23em}.kv{margin-top:1.34em}.mu{margin-top:1.41em}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="print">.ab{display:none}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.y{transition:transform 300ms ease}.z{will-change:transform}.nj{transition:opacity 200ms}.ov{transition:border-color 150ms ease}.ow::before{background:
      radial-gradient(circle, rgba(90, 118, 144, 1) 60%, transparent 70%)
    }.ox::before{border-radius:50%}.oy::before{content:""}.oz::before{display:block}.pa::before{z-index:0}.pb::before{left:0}.pc::before{height:100%}.pd::before{position:absolute}.pe::before{top:0}.pf::before{width:100%}.pg:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.pj{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 1230px)">.nk{display:none}</style><style type="text/css" data-fela-rehydration="491" data-fela-type="RULE" media="all and (max-width: 1198px)">.nm{display:none}</style><script charset="utf-8" src="./A gentle introduction to OCR - Towards Data Science_files/vendors_tracing.a5df20bc.chunk.js"></script><script charset="utf-8" src="./A gentle introduction to OCR - Towards Data Science_files/tracing.45a59d0a.chunk.js"></script><link rel="icon" href="https://miro.medium.com/fit/c/128/128/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*77Hn7O_KROhi37Gt7Aq67Q.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-gentle-introduction-to-ocr-ee1469a201aa","dateCreated":"2018-10-22T07:16:24.241Z","datePublished":"2018-10-22T07:16:24.241Z","dateModified":"2019-06-25T08:55:17.433Z","headline":"A gentle introduction to OCR","name":"A gentle introduction to OCR","description":"OCR, or optical character recognition, is one of the earliest addressed computer vision tasks, since in some aspects it does not require deep learning. Therefore there were different OCR…","identifier":"ee1469a201aa","keywords":["Lite:true","Tag:Machine Learning","Tag:Artificial Intelligence","Tag:Ocr","Tag:Computer Vision","Tag:Deep Learning","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:4"],"author":{"@type":"Person","name":"Gidi Shperber","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@gidishperber"},"creator":["Gidi Shperber"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F165\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-gentle-introduction-to-ocr-ee1469a201aa"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y z ab" style="transform: translateY(-100%);"><div class="branch-journeys-top"><div class="r c"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="al n o am an"><div class="n o ao w"><a href="https://medium.com/?source=post_page-----ee1469a201aa----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div><div class="r ap w"><span class="aq b ar as at au r av aw"><div class="n o ax"><div class="n f"><div class="bf" aria-hidden="true"><div class="n"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25" class="ce cf r cg ch"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="bg bh bi bj as bk bl bm bn" placeholder="Search Towards Data Science" value=""></div></div></div><div class="ci cj"><a href="https://towardsdatascience.com/search?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ce ck r cg ch"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a href="https://medium.com/me/list/queue?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ck r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="ck n ch"><div class="bf" aria-hidden="true"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd r"><svg width="25" height="25" viewBox="-293 409 25 25" class="rx r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="ib" id="li-post-page-navbar-upsell-button"><div class="ck r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="cm cn q co cp cq cr cs bx cc ct cu cv cw cx cy cz aq b ar as at au da db dc bf dd ca" rel="noopener">Upgrade</a></div></div></div><div class="n" aria-hidden="true"><div class="de df n o"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><img alt="Johnny Chen" class="r dg dh di" src="./A gentle introduction to OCR - Towards Data Science_files/1_dmbNkD5D-u45r44go_cf0g.png" width="32" height="32"></button></div></div></div></span></div></div></div></div></div><div class="dj r dk dl"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="dm bd n o"><div class="dn r ap"><a href="https://towardsdatascience.com/?source=post_page-----ee1469a201aa----------------------" rel="noopener"><div class="do dp r"><img alt="Towards Data Science" class="" src="./A gentle introduction to OCR - Towards Data Science_files/1_mG6i4Bh_LgixUYXJgQpYsg@2x.png" width="100" height="36"></div></a></div><div class="dq r dr"><ul class="ds bw dt be du n dv g dw dx dy"><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Data Science</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Machine Learning</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/programming/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Programming</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Visualization</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">AI</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/our-picks/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Picks</a></span></li><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/more/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">More</a></span></li><span class="dz ed ee"></span><li class="n o dz ea eb ec"><span class="aq ef eg as eh ei ej"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ek el ca cb em en" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="eo ep r eq er"></div><article><section class="es et eu ev ak ew dc n ex"></section><span class="r"></span><div><div class="ey u ez fa fb fc"></div><div class="eu ev ew bl"><div class="r h g f e"><aside class="sa ey t" style="width: 571.5px;"><div class="sd se ey sf be ak"><h4 class="aq ef eg as av"><span class="bf se be bd bc">Top highlight</span></h4></div></aside></div></div><section class="fd fe ff fg fh"><div class="n p"><div class="ac ae af ag ah fi aj ak"><div><div id="4502" class="fj fk cn ar fl b fm fn fo fp fq fr fs ft fu fv fw"><h1 class="fl b fm fx fo fy fq fz fs ga fu gb cn">A gentle introduction to OCR</h1></div><div class="gc"><div class="n gd ge gf gg"><div class="o n"><div><a href="https://towardsdatascience.com/@gidishperber?source=post_page-----ee1469a201aa----------------------" rel="noopener"><img alt="Gidi Shperber" class="r dg gh gi" src="./A gentle introduction to OCR - Towards Data Science_files/0_F1qzwbKctxWXpQhr.jpg" width="48" height="48"></a></div><div class="gj ak r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r cn q"><div class="gk n o gl"><span class="aq ef bj as bd gm bc gn go gp cn"><a href="https://towardsdatascience.com/@gidishperber?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx gq ca cb cc cd" rel="noopener">Gidi Shperber</a></span><div class="gr r ap h"><button class="gs co gt gu gv gw gx gy bx cz aq b ar gz eg au da db dc bf dd ca">Follow</button></div></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq ef bj as bd gm bc gn go gp av"><div><a class="bo bp bq br bs bt bu bv bw bx gq ca cb cc cd" rel="noopener" href="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa?source=post_page-----ee1469a201aa----------------------">Oct 22, 2018</a> <!-- -->·<!-- --> <!-- -->16<!-- --> min read</div></span></span></div></div><div class="n ha hb hc hd he hf hg hh ab"><div class="n o"><div class="hi r ap"><a href="https://medium.com/p/ee1469a201aa/share/twitter?source=post_actions_header---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hi r ap"><a href="https://medium.com/p/ee1469a201aa/share/facebook?source=post_actions_header---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="hj r ao"><div><div class="hk"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev hl"><div class="hw r bl hx"><div class="hy r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_77Hn7O_KROhi37Gt7Aq67Q.png" width="448" height="362" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="448" height="362" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_77Hn7O_KROhi37Gt7Aq67Q(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/896/1*77Hn7O_KROhi37Gt7Aq67Q.png" width="448" height="362" role="presentation"/></noscript></div></div></div></figure><blockquote class="id"><div id="4457" class="ie if ig ar fl b ih ii ij ik il im in" data-selectable-paragraph=""><p class="fl b io ip av">Want to learn more? visit <a href="https://www.shibumi-ai.com/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">www.Shibumi-ai.com</a></p></div></blockquote><h1 id="5937" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Intro</h1><p id="c413" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph=""><strong class="ji jt">OCR</strong>, or optical character recognition, is one of the earliest addressed computer vision tasks, since in some aspects it does not require deep learning. Therefore there were different OCR implementations even before the deep learning boom in 2012, and some even dated back to 1914 (!).</p><p id="d0fa" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">This makes many people think the OCR challenge is <strong class="ji jt">“solved</strong>”, it is no longer challenging. Another belief which comes from similar sources is that OCR does not require <strong class="ji jt">deep learning</strong>, or in other words, using deep learning for OCR is an overkill.</p><p id="d1fc" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Anyone who practices computer vision, or machine learning in general, knows that there is no such thing as a solved task, and this case is not different. On the contrary, OCR yields very-good results only on very specific use cases, but in general, it is still considered as challenging.</p><p id="70df" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Additionally, it’s true there are good solutions for certain OCR tasks that do not require deep learning. However, to really step forward towards better, more general solutions, deep learning will be mandatory.</p><h2 id="f0e8" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">why do I write about OCR?</h2><p id="5700" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As many of my works/write-ups, this too started off as project for client. I was requested to solve a specific OCR task.</p><p id="2874" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">During and after working on this task, I’ve reached some conclusions and insights which I was very eager to share with you. Additionally, after intensively working on such a task, it is hard for me to stop and throw it away, so I keep my research going, and hoping to achieve an even better and more generalized solution.</p><h2 id="be65" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">What you’ll find here</h2><p id="5c98" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">In this post I will explore some of the <strong class="ji jt">strategies</strong>, <strong class="ji jt">methods</strong> and <strong class="ji jt">logic</strong> used to address different OCR tasks, and will share some useful approaches. In the last part, we will tackle a <strong class="ji jt">real world problem</strong> with code. This should not be considered as an exhaustive review (unfortunately) since the depth, history and breadth of approaches are too wide for this kind of a blog-post.</p><p id="df3c" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">However, as always, I will not spare you from references to articles, data sets, repositories and other relevant blog-posts.</p><h1 id="d197" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Types of OCR</h1><p id="b507" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As I hinted before, there are more than one meaning for OCR. In its most general meaning, it refers to extracting text from every possible image, be it a standard printed page from a book, or a random image with graffiti in it (“<strong class="ji jt">in the wild</strong>”). In between, you may find many other tasks, such as reading <strong class="ji jt">license plates</strong>, no-robot <strong class="ji jt">captchas</strong>, <strong class="ji jt">street signs</strong> etc.</p><p id="a216" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Although each of these options has its own difficulties, clearly “in the wild” task is the hardest.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev fi"><div class="hw r bl hx"><div class="kl r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_n6BoysCl4xSmMHcDwqK_ZA.png" width="680" height="316" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="680" height="316" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_n6BoysCl4xSmMHcDwqK_ZA(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1360/1*n6BoysCl4xSmMHcDwqK_ZA.png" width="680" height="316" role="presentation"/></noscript></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">Left: Printed text. Right: text in the wild</figcaption></figure><p id="de5f" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Form these examples we can draw out some <strong class="ji jt">attributes</strong> of the OCR tasks:</p><ul class=""><li id="a33e" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Text density</strong>: on a printed/written page, text is dense. However, given an image of a street with a single street sign, text is sparse.</li><li id="bae1" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Structure of text</strong>: text on a page is structured, mostly in strict rows, while text in the wild may be sprinkled everywhere, in different rotations.</li><li id="8327" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Fonts</strong>: printed fonts are easier, since they are more structured then the noisy hand-written characters.</li><li id="85ba" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Character type: </strong>text may come in different language which may be very different from each other. Additionally, structure of text may be different from numbers, such as house numbers etc.</li><li id="1908" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Artifacts</strong>: clearly, outdoor pictures are much noisier than the comfortable scanner.</li><li id="77fd" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Location</strong>: some tasks include cropped/centered text, while in others, text may be located in random locations in the image.</li></ul><h1 id="aa2c" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Data sets/Tasks</h1><h2 id="7332" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">SVHN</h2><p id="8bb0" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">A good place to start from is <a href="http://ufldl.stanford.edu/housenumbers/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">SVHN</a>, Street View House Numbers data-set. As its name implies, this is a data-set of house numbers extracted from google street view. The task difficulty is intermediate. The digits come in various shapes and writing styles, however, each house number is located in the middle of the image, thus detection is not required. The images are not of a very high resolution, and their arrangement may be a bit peculiar.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="la lb bl lc ak"><div class="eu ev kz"><div class="hw r bl hx"><div class="ld r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_wa6uLBFTeg9rcSoC" width="1091" height="728" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="1091" height="728" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_wa6uLBFTeg9rcSoC(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/2182/0*wa6uLBFTeg9rcSoC" width="1091" height="728" role="presentation"/></noscript></div></div></div></div></figure><h2 id="88a3" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">License plates</h2><p id="bed6" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Another common challenge, which is not very hard and useful in practice, is the license plate recognition. This task, as most OCR tasks, requires to detect the license plate, and then recognizing it’s <strong class="ji jt">characters</strong>. Since the plate’s shape is relatively constant, some approach use simple reshaping method before actually recognizing the digits. Here are some examples from the web:</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="la lb bl lc ak"><div class="eu ev le"><div class="hw r bl hx"><div class="lf r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_3cGk3lVSAiaRAnPCqDCEEw.png" width="782" height="253" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="782" height="253" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_3cGk3lVSAiaRAnPCqDCEEw(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1564/1*3cGk3lVSAiaRAnPCqDCEEw.png" width="782" height="253" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">OpenALPR example. with car type a s abonus</figcaption></figure><ol class=""><li id="6a75" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in lg ks kt" data-selectable-paragraph=""><a href="https://github.com/openalpr/openalpr" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">OpenALPR</a> is a very robust tool, with no deep learning involved, to recognize license plates from various countries</li><li id="fb5f" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">This <a href="https://github.com/qjadud1994/CRNN-Keras" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">repo</a> provides an implementation of CRNN model (will be further discussed) to recognize Korean license plates.</li><li id="9488" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">Supervise.ly, a data utilities company, <a class="bo dd iq ir is it" target="_blank" rel="noopener" href="https://towardsdatascience.com/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c">wrote</a> about training a license plate recognizer using artificial data generated by their tool (artificial data will also be further discussed)</li></ol><h2 id="3195" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">CAPTCHA</h2><p id="7009" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Since the internet is full of robots, a common practice to tell them apart from real humans, are vision tasks, specifically text reading,&nbsp;aka&nbsp;CAPTCHA. Many of these texts are random and distorted, which should make it harder for computer to read. I’m not sure whoever developed the CAPTCHA predicted the advances in computer vision, however most of today text CAPTCHAs are not very hard to solve, especially if we don’t try to solve all of them at once.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lh"><div class="hw r bl hx"><div class="li r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_sRzszQ-SBtcGW3KyQwJtEQ.png" width="472" height="253" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="472" height="253" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_sRzszQ-SBtcGW3KyQwJtEQ(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/944/1*sRzszQ-SBtcGW3KyQwJtEQ.png" width="472" height="253" role="presentation"/></noscript></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">Facebook knows how to make challenging CAPTCHAs</figcaption></figure><p id="25a7" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Adam Geitgey provides a <a href="https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710" class="bo dd iq ir is it" target="_blank" rel="noopener">nice tutorial</a> to solving some CAPTCHAs with deep learning, which includes synthesizing artificial data once again.</p><h2 id="1181" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">PDF OCR</h2><p id="a370" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">The most common scenario for OCR is the printed/pdf OCR. The structured nature of printed documents make it much easier to parse them. Most OCR tools (e.g <a href="https://github.com/tesseract-ocr/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">Tesseract</a>) are mostly intended to address this task, and achieve good result. Therefore, I will not elaborate too much on this task in this post.</p><h2 id="eb27" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">OCR in the wild</h2><p id="2fc0" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">This is the most challenging OCR task, as it introduces all general computer vision challenges such as noise, lighting, and artifacts into OCR. Some relevant data-sets for this task is the <a href="https://vision.cornell.edu/se3/coco-text-2/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">coco-text</a>, and the <a href="http://tc11.cvc.uab.es/datasets/SVT_1" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">SVT</a> data set which once again, uses street view images to extract text from.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lj"><div class="hw r bl hx"><div class="lk r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_h1CDmAxHf1Ee0illDor2HA.png" width="526" height="369" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="526" height="369" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_h1CDmAxHf1Ee0illDor2HA(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1052/1*h1CDmAxHf1Ee0illDor2HA.png" width="526" height="369" role="presentation"/></noscript></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">COCO text example</figcaption></figure><h2 id="06b0" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">Synth text</h2><p id="97de" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph=""><strong class="ji jt">SynthText</strong> is not a data-set, and perhaps not even a task, but a nice idea to improve training efficiency is artificial data generation. Throwing random characters or words on an image will seem much more natural than any other object, because of the flat nature of text.</p><p id="9a53" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">We have seen earlier some data generation for easier tasks like CAPTCHA and license plate. Generating text in the wild is a little bit more complex. The task includes considering depth information of an image. Fortunately, SynthText is a nice work that takes in images with the aforementioned annotations, and intelligently sprinkles words (from newsgroup data-set).</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev ll"><div class="hw r bl hx"><div class="lm r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/1_GLFRyZ99LJHmCewUqhKLdA.png" width="633" height="530" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="633" height="530" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/1_GLFRyZ99LJHmCewUqhKLdA(1).png"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1266/1*GLFRyZ99LJHmCewUqhKLdA.png" width="633" height="530" role="presentation"/></noscript></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">SynthText process illustration: top right is the segmentation of an image, bottom right is the depth data. Bottom left is a surface analyses of the image, which according to text is sprinkled on the image.</figcaption></figure><p id="8156" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">To make the “sprinkled” text look realistic and useful, the SynthText library takes with every image two masks, one of depth and another of segmentation. If you like to use your own images, you should add this data as well</p><ul class=""><li id="b99b" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in kr ks kt" data-selectable-paragraph="">It is recommended to check the <a href="https://github.com/ankush-me/SynthText" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">repo</a> and generate some images on your own. You should pay attention that the repo uses some outdated version of opencv and maptlotlib, so some modifications may be necessary.</li></ul><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev ln"><div class="hw r bl hx"><div class="lo r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_xs7V0gNwOdgfJkP6" width="468" height="328" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="468" height="328" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_xs7V0gNwOdgfJkP6(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/936/0*xs7V0gNwOdgfJkP6" width="468" height="328" role="presentation"/></noscript></div></div></div></figure><h2 id="3abd" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">Mnist</h2><p id="7be3" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Although not really an OCR task, it is impossible to write about OCR and not include the Mnist example. The most well known computer vision challenge is not really an considered and OCR task, since it contains one character (digit) at a time, and only 10 digits. However, it may hint why OCR is considered easy. Additionally, in some approaches every letter will be detected separately, and then Mnist like (classification) models become relevantץ</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lp"><div class="hw r bl hx"><div class="lq r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_JdoUIidwCrrUZPPc" width="534" height="49" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="534" height="49" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_JdoUIidwCrrUZPPc(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1068/0*JdoUIidwCrrUZPPc" width="534" height="49" role="presentation"/></noscript></div></div></div></figure><h1 id="4dd0" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Strategies</h1><p id="dc14" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As we’ve seen and implied, the text recognition is mostly a two-step task. First, you would like to <strong class="ji jt">detect</strong> the text(s) appearances in the image, may it be dense (as in printed document) or sparse (As text in the wild).</p><p id="e06b" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">After detecting the line/word level we can choose once again from a large set of solutions, which generally come from three main approaches:</p><ol class=""><li id="ce04" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in lg ks kt" data-selectable-paragraph="">Classic computer vision techniques.</li><li id="229a" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">Specialized deep learning.</li><li id="f73f" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">Standard deep learning approach (Detection).</li></ol><p id="94e1" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Let’s examine each of them:</p><h2 id="4016" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">1. Classic computer vision techniques</h2><p id="e187" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As said earlier, computer vision solves various text recognition problems for a long time. You can find many examples online:</p><ul class=""><li id="e1ae" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in kr ks kt" data-selectable-paragraph="">The Great <strong class="ji jt">Adrian Rosebrook</strong> has a tremendous number of tutorials in his site, like this <a href="https://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">one</a>, this <a href="https://www.pyimagesearch.com/2017/07/24/bank-check-ocr-with-opencv-and-python-part-i/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">one</a> and <a href="https://www.pyimagesearch.com/category/optical-character-recognition-ocr/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">more</a>.</li><li id="5bf5" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph=""><strong class="ji jt">Stack overflow</strong> has also some gems like <a href="https://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">this</a> one.</li></ul><p id="8c06" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The classic-CV approach generally claims:</p><ol class=""><li id="1d33" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in lg ks kt" data-selectable-paragraph="">Apply <strong class="ji jt">filters</strong> to make the characters stand out from the background.</li><li id="0ff0" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">Apply <strong class="ji jt">contour detection</strong> to recognize the characters one by one.</li><li id="bbda" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in lg ks kt" data-selectable-paragraph="">Apply <strong class="ji jt">image classification</strong> to identify the characters</li></ol><p id="2b5b" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Clearly, if part two is done well, part three is easy either with pattern matching or machine learning (e.g Mnist).</p><p id="e4ce" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">However, the contour detection is quite challenging for generalization. it requires a lot of manual fine tuning, therefore becomes infeasible in most of the problem. e.g, lets apply a simple computer vision script from <a href="http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html#sphx-glr-download-auto-examples-segmentation-plot-label-py" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a> on some images from SVHN data-set. At first attempt we may achieve very good results:</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lr"><div class="hw r bl hx"><div class="ls r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_R6r8eGNyoylNVj4G" width="299" height="252" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="299" height="252" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_R6r8eGNyoylNVj4G(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/598/0*R6r8eGNyoylNVj4G" width="299" height="252" role="presentation"/></noscript></div></div></div></figure><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lt"><div class="hw r bl hx"><div class="lu r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_PMmcCq_CtcwIYg0x" width="280" height="236" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="280" height="236" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_PMmcCq_CtcwIYg0x(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/560/0*PMmcCq_CtcwIYg0x" width="280" height="236" role="presentation"/></noscript></div></div></div></figure><p id="7dca" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">But when characters are closer to each other, things start to break:</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lr"><div class="hw r bl hx"><div class="ls r"><div class="bk hs ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia cl sg" src="./A gentle introduction to OCR - Towards Data Science_files/0_HwWJowf1UobFjbNR" width="299" height="252" role="presentation"></div><img class="qf rz ey t u ht ak ic" width="299" height="252" role="presentation" src="./A gentle introduction to OCR - Towards Data Science_files/0_HwWJowf1UobFjbNR(1)"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/598/0*HwWJowf1UobFjbNR" width="299" height="252" role="presentation"/></noscript></div></div></div></figure><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev lt"><div class="hw r bl hx"><div class="lu r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/0_lVny6F7mu7wzhFAG" width="280" height="236" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="280" height="236" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/560/0*lVny6F7mu7wzhFAG" width="280" height="236" role="presentation"/></noscript></div></div></div></figure><p id="0db6" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">I’ve found out the hard way, that when you start messing around with the parameters, you may reduce such errors, but unfortunately cause others. In other words, if your task is not straightforward, these methods are not the way to go.</p><h2 id="940f" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">2. Specialized deep learning approaches</h2><p id="2cba" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Most successful deep learning approaches excel in their generality. However, considering the attributes described above, Specialized networks can be very useful.</p><p id="1189" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">I’ll examine here an unexhaustive sample of some prominent approaches, and will do a very quick summary of the articles which present them. As always, every article is opened with the words “task X (text recognition) gains attention lately” and goes on to describe their method in detail. Reading the articles carefully will reveal these methods are assembled from pieces of previous deep learning/text recognition works.</p><p id="32dd" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Results are also depicted thoroughly, however due to many differences in design (including minor differences in data sets)actual comparison is quite impossible. The only way to actually know the performance of these methods on your task, is to get their code (best to worse: find <strong class="ji jt">official</strong> repo, find <strong class="ji jt">unofficial but highly rated</strong> repo, <strong class="ji jt">implement</strong> by yourself) and try it on your data.</p><p id="42ba" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Thus, we will always prefer articles with good accompanying repos, and even demos if possible.</p><p id="b9cd" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><strong class="ji jt">EAST</strong></p><p id="583c" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1704.03155.pdf" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">EAST</a> ( Efficient accurate scene text detector) is a simple yet powerful approach for <strong class="ji jt">text detection</strong>. Using a specialized network.</p><p id="5de3" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Unlike the other methods we’ll discuss, is limited only to text detection (not actual recognition) however it’s robustness make it worth mentioning.<br> Another advantage is that it was also added to <strong class="ji jt">open-CV </strong>library (from version 4) so you can easily use it (see tutorial <a href="https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>).<br> The network is actually a version of the well known <strong class="ji jt">U-Net</strong>, which good for detecting features that may vary in size. The underlying feed forward “stem” (as coined in the article, see figure below) of this network may very — <strong class="ji jt">PVANet</strong> is used in the paper, however opencv implementation use <strong class="ji jt">Resnet</strong>. Obviously, it can be also pre-trained (with imagenet e.g)&nbsp;. As in U-Net, features are extracted from different levels in the network.</p></div></div></section><hr class="lv ef lw bg lx ko ly lz ma mb mc"><section class="fd fe ff fg fh"><div class="n p"><div class="ac ae af ag ah fi aj ak"><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="la lb bl lc ak"><div class="eu ev md"><div class="hw r bl hx"><div class="me r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_Kd5tAQp83XY2udt8BE1d0A.png" width="363" height="383" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="363" height="383" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/726/1*Kd5tAQp83XY2udt8BE1d0A.png" width="363" height="383" role="presentation"/></noscript></div></div></div></div></figure><p id="749d" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Finally, the network allows two types of outputs rotated bounding boxes: either a standard bounding box with a rotation angle (2X2+1 parameters) or “quadrangle” which is merely a rotated bounding box with coordinates of all vertices.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev mf"><div class="hw r bl hx"><div class="mg r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_BVae8WaaVsznWP9cVHJotw.png" width="458" height="177" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="458" height="177" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/916/1*BVae8WaaVsznWP9cVHJotw.png" width="458" height="177" role="presentation"/></noscript></div></div></div></figure><p id="8a03" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">If real life results will be as in the above images, recognizing the texts will not take much of an effort. However, real life results are not perfect.</p><p id="d370" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><strong class="ji jt">CRNN</strong></p><p id="9bfc" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Convolutional-recurrent neural network, is an article from 2015, which suggest a hybrid (or tribrid?) end to end architecture, that is intended to capture words, in a three step approach.</p><p id="c34f" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The idea goes as follows: the first level is a standard fully convolutional network. The last layer of the net is defined as feature layer, and divided into “feature columns”. See in the image below how every such feature column is intended to represent a certain section in the text.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev mh"><div class="hw r bl hx"><div class="mi r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_5QhwZ7bChMaTsI8ZRvCY_w.png" width="341" height="244" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="341" height="244" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/682/1*5QhwZ7bChMaTsI8ZRvCY_w.png" width="341" height="244" role="presentation"/></noscript></div></div></div></figure><p id="234d" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Afterwards, the feature columns are fed into a deep-bidirectional LSTM which outputs a sequence, and is intended for finding relations between the characters.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev mj"><div class="hw r bl hx"><div class="mk r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/0_nGWtig3Cd0Jma2nX" width="447" height="524" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="447" height="524" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/894/0*nGWtig3Cd0Jma2nX" width="447" height="524" role="presentation"/></noscript></div></div></div></figure><p id="ef48" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Finally, the third part is a transcription layer. Its goal is to take the messy character sequence, in which some characters are redundant and others are blank, and use probabilistic method to unify and make sense out of it.</p><p id="e6d8" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">This method is called <strong class="ji jt">CTC loss</strong>, and can be read about <a href="https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>. This layer can be used with/without predefined lexicon, which may facilitate predictions of words.</p><p id="8cd4" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">This paper reaches high (&gt;95%) rates of accuracy with fixed text lexicon, and varying rates of success without it.</p><h2 id="7ccd" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">STN-net/SEE</h2><p id="66ff" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph=""><a href="https://arxiv.org/pdf/1712.05404.pdf" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">SEE</a> — Semi-Supervised End-to-End Scene Text Recognition, is a work by Christian Bartzi. He and his colleagues apply a truly end to end strategy to detect and recognize text. They use very weak supervision (which they refer to as semi-supervision, in a different meaning than usual ). as they train the network with <strong class="ji jt">only text annotation</strong> (without bounding boxes). This allows them use more data, but makes their training procedure quite challenging, and they discuss different tricks to make it work, e.g not training on images with more than two lines of text (at least at the first stages of training).</p><p id="2a32" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The paper has an earlier version which is called <a href="https://arxiv.org/abs/1707.08831" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">STN OCR</a>. In the final paper the researchers have refined their methods and presentation, and additionally they’ve put more emphasis on the generality of their approach on the account of high quality of the results.</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="la lb bl lc ak"><div class="eu ev le"><div class="hw r bl hx"><div class="ml r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_XAUtH9C1iPLa9clk9RA-8Q.png" width="782" height="239" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="782" height="239" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1564/1*XAUtH9C1iPLa9clk9RA-8Q.png" width="782" height="239" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">SEE strategy</figcaption></figure><p id="bb07" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The name <strong class="ji jt">STN-OCR</strong> hints on the strategy, of using <a href="https://arxiv.org/pdf/1506.02025.pdf" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">spatial transformer</a> (=STN, no relation to the recent google transformer).</p><p id="7178" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">They train <strong class="ji jt">two concatenated networks </strong>in<strong class="ji jt"> </strong>which the first network, the transformer, learns a transformation on the image to output an easier sub-image to interpret.</p><p id="7b4c" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Then, another feed forward network with LSTM on top (hmm… seems like we’ve seen it before) to recognize the text.</p><p id="745b" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The researches emphasize here the importance of using resnet(they use it twice) since it provides “strong” propagation to the early layers. however this practice quite accepted nowadays.</p><p id="5c90" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Either way, this is an interesting approach to try.</p><h2 id="ccf6" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">3. Standard deep learning approach</h2><p id="1500" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As the header implies, after detecting the “words” we can apply standard deep learning detection approaches, such as SSD, YOLO and Mask RCNN. I’m not going to elaborate too much on theses approaches since there is a plethora of info online.</p><p id="8647" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">I must say this is currently my favorite approach, since what I like in deep learning is the “end to end” philosophy, where you apply a strong model which with some tuning will solve almost every problem. In the next section of this post we will see how it actually works.</p><p id="c122" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">However, SSD and other detection models are challenged when it comes to dense, similar classes, as reviewed <a href="https://arxiv.org/pdf/1611.10012.pdf" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>. <mark class="sb sc oc">I find it a bit ironic since in fact, deep learning models find it much more difficult to recognize digits and letters than to recognize much more challenging and elaborate objects such as dogs, cats or humans.</mark> They tend no to reach the desired accuracy, and therefore, specialized approaches thrive.</p><h1 id="0411" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Practical Example</h1><p id="8670" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">So after all the talking, it’s time to get our hands dirty, and try some modelling ourselves. We will try the tackle <a href="http://ufldl.stanford.edu/housenumbers/" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">SVHN</a> task. The SVHN data contains three different data-sets: <em class="mm">train</em>, <em class="mm">test</em> and <em class="mm">extra</em>. The differences are not 100% clear, however the <em class="mm">extra</em> data-set which is the biggest (with ~500K samples) includes images that are somehow easier to recognize. So for the sake of this take we will use it.</p><p id="d127" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">To prepare for the task, do the following:</p><ul class=""><li id="2a8f" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in kr ks kt" data-selectable-paragraph="">You’ll need a basic GPU machine with Tensorflow≥1.4 and Keras≥2</li><li id="6cc3" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph="">Clone the SSD_Keras project from <a href="https://github.com/pierluigiferrari/ssd_keras" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>.</li><li id="a494" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph="">Download the pre-trained SSD300 model on coco data-set from <a href="https://drive.google.com/open?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>.</li><li id="785b" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph="">Clone <em class="mm">this</em> project's repo from <a href="https://github.com/shgidi/OCR" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>..</li><li id="d9cc" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph="">Download the <a href="http://ufldl.stanford.edu/housenumbers/extra.tar.gz" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">extra.tar.gz</a> file, which contains the extra images of SVHN data-set.</li><li id="9d2b" class="jg jh cn ar ji b jj ku jl kv jn kw jp kx jr ky in kr ks kt" data-selectable-paragraph="">Update all relevant paths in json_config.json in this project repo.</li></ul><p id="a7d3" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">To efficiently follow the process, you should read the below instruction along with running the <strong class="ji jt">ssd_OCR.ipynb</strong> notebook from the project’s repo.</p><p id="a7a6" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">And… You are ready to start!</p><h2 id="31c1" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">Step 1: parse the data</h2><p id="a1a7" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Like it or not, but there is no “golden” format for data representation in detection tasks. Some well known formats are: coco, via, pascal, xml. And there are more. For instance, the SVHN data-set is annotated with the obscure <em class="mm">.mat </em>format. Fortunately for us, this <a href="https://gist.github.com/veeresht/7bf499ee6d81938f8bbdb3c6ef1855bf" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">gist</a> provides a slick <em class="mm">read_process_h5</em> script to convert the .mat file to standard json, and you should go one step ahead and convert it further to pascal format, like so:</p><pre class="hm hn ho hp hq mn mo du"><span id="9d2c" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">def json_to_pascal(json, filename): #filename is the .mat file<br>    # convert json to pascal and save as csv<br>    pascal_list = []<br>    for i in json:<br>        for j in range(len(i['labels'])):<br>            pascal_list.append({'fname': i['filename'] <br>            ,'xmin': int(i['left'][j]), 'xmax': int(i['left'][j]+i['width'][j])<br>            ,'ymin': int(i['top'][j]),  'ymax': int(i['top'][j]+i['height'][j])<br>            ,'class_id': int(i['labels'][j])})<br>    df_pascal = pd.DataFrame(pascal_list,dtype='str')<br>    df_pascal.to_csv(filename,index=False)</span><span id="3c81" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph=""><em class="mm">p = read_process_h5(file_path)</em></span><span id="e507" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">json_to_pascal(p, data_folder+'pascal.csv')</span></pre><p id="03b3" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Now we should have a <em class="mm">pascal.csv</em> file that is much more standard and will allow us to progress. If the conversion is to slow, you should take note that we don’t need t all the data samples. ~10K will be enough.</p><h2 id="00eb" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">Step 2: look at the data</h2><p id="60a3" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">Before starting the modeling process, you should better do some exploration of the data. I only provide a quick function for sanity test, but I recommend you to do some further analysis:</p><pre class="hm hn ho hp hq mn mo du"><span id="5a11" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">def viz_random_image(df):<br>    file = np.random.choice(df.fname)<br>    im = skimage.io.imread(data_folder+file)<br>    annots =  df[df.fname==file].iterrows()</span><span id="c064" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">    plt.figure(figsize=(6,6))<br>    plt.imshow(im)</span><span id="add4" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">    current_axis = plt.gca()</span><span id="6e5b" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">    for box in annots:<br>        label = box[1]['class_id']<br>        current_axis.add_patch(plt.Rectangle(<br>            (box[1]['xmin'], box[1]['ymin']), box[1]['xmax']-box[1]['xmin'],<br>            box[1]['ymax']-box[1]['ymin'], color='blue', fill=False, linewidth=2))  <br>        current_axis.text(box[1]['xmin'], box[1]['ymin'], label, size='x-large', color='white', bbox={'facecolor':'blue', 'alpha':1.0})<br>        plt.show()<br></span><span id="9ba3" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">viz_random_image(df)</span></pre><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev my"><div class="hw r bl hx"><div class="mz r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_zLCfj1HVgil2JRywTQgjew.png" width="297" height="139" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="297" height="139" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/594/1*zLCfj1HVgil2JRywTQgjew.png" width="297" height="139" role="presentation"/></noscript></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">A representative sample form SVHN dataset</figcaption></figure><p id="f0aa" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">For the following steps, I provide a <em class="mm">utils_ssd.py</em> in the repo that facilitates the training, weight loading etc. Some of the code is taken from the SSD_Keras repo, which is also used extensively.</p><h2 id="c228" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">Step 3: choosing strategy</h2><p id="29cd" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">As previously discussed, we have many possible approaches for this problem. In this tutorial I’ll take standard deep learning detection approach, and will use the SSD detection model. We will use the <strong class="ji jt">SSD keras</strong> implementation from <a href="https://github.com/pierluigiferrari/ssd_keras" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">here</a>. This is a nice Implementation by PierreLuigi. Although it has less GitHub stars than the <a href="https://github.com/rykov8/ssd_keras" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">rykov8</a> implementation, it seems more updated, and is easier to integrate. This is a very important thing to notice when you choose which project are you going to use. Other good choices will be the YOLO model, and the Mask RCNN.</p><h2 id="bff4" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph=""><strong class="bu">Step 4: Load and train the SSD model</strong></h2><p id="5f0e" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph=""><strong class="ji jt">Some definitions</strong></p><p id="f701" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">To use the repo, you’ll need to verify you have the SSD_keras repo, and fill in the paths in the json_config.json file, to allow the notebook finding the paths.</p><p id="c4d0" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Start with importing:</p><pre class="hm hn ho hp hq mn mo du"><span id="a09a" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph=""><strong class="mp jt">import</strong> <strong class="mp jt">os</strong><br><strong class="mp jt">import</strong> <strong class="mp jt">sys</strong><br><strong class="mp jt">import</strong> <strong class="mp jt">skimage.io</strong><br><strong class="mp jt">import</strong> <strong class="mp jt">scipy</strong><br><strong class="mp jt">import</strong> <strong class="mp jt">json</strong></span><span id="e787" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph=""><strong class="mp jt">with</strong> open('json_config.json') <strong class="mp jt">as</strong> f:     json_conf = json.load(f)</span><span id="6388" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">ROOT_DIR = os.path.abspath(json_conf['ssd_folder']) <em class="mm"># add here mask RCNN path</em><br>sys.path.append(ROOT_DIR)<br><br><strong class="mp jt">import</strong> <strong class="mp jt">cv2</strong><br><strong class="mp jt">from</strong> <strong class="mp jt">utils_ssd</strong> <strong class="mp jt">import</strong> *<br><strong class="mp jt">import</strong> <strong class="mp jt">pandas</strong> <strong class="mp jt">as</strong> <strong class="mp jt">pd</strong><br><strong class="mp jt">from</strong> <strong class="mp jt">PIL</strong> <strong class="mp jt">import</strong> Image<br><br><strong class="mp jt">from</strong> <strong class="mp jt">matplotlib</strong> <strong class="mp jt">import</strong> pyplot <strong class="mp jt">as</strong> plt<br><br>%matplotlib inline<br>%load_ext autoreload<br>% autoreload 2</span></pre><p id="adab" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">and some more definitions:</p><pre class="hm hn ho hp hq mn mo du"><span id="b1f5" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">task = 'svhn'</span><span id="7a8d" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">labels_path = f'{data_folder}pascal.csv'</span><span id="b145" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">input_format = ['class_id','image_name','xmax','xmin','ymax','ymin' ]<br>    <br>df = pd.read_csv(labels_path)</span></pre><p id="e458" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><strong class="ji jt">Model configurations</strong>:</p><pre class="hm hn ho hp hq mn mo du"><span id="dd10" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph=""><strong class="mp jt">class</strong> <strong class="mp jt">SVHN_Config</strong>(Config):<br>    batch_size = 8<br>    <br>    dataset_folder = data_folder<br>    task = task<br>    <br>    labels_path = labels_path<br><br>    input_format = input_format<br><br>conf=SVHN_Config()<br><br>resize = Resize(height=conf.img_height, width=conf.img_width)<br>trans = [resize]</span></pre><p id="053f" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><strong class="ji jt">Define the model, load weights</strong></p><p id="bbd9" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">As in most of deep learning cases, we won’t start training from scratch, but we’ll load pre-trained weights. In this case, we’ll load the <a href="https://drive.google.com/open?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj" class="bo dd iq ir is it" target="_blank" rel="noopener nofollow">weights</a> of SSD model, trained on COCO data-set, which has 80 classes. Clearly our task has only 10 classes, therefore we will reconstruct the top layer to have the right number of outputs, after our loading the weights. We do it in the init_weights function. A side note: the right number of outputs in this case is 44: 4 for each class (bounding box coordinates) and another 4 for the background/none class.</p><pre class="hm hn ho hp hq mn mo du"><span id="d011" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">learner = SSD_finetune(conf)<br>learner.get_data(create_subset=<strong class="mp jt">True</strong>)<br><br>weights_destination_path=learner.init_weights()<br><br>learner.get_model(mode='training', weights_path = weights_destination_path)<br>model = learner.model<br>learner.get_input_encoder()<br>ssd_input_encoder = learner.ssd_input_encoder<br><br><em class="mm"># Training schedule definitions</em><br>adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) <br>ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)<br>model.compile(optimizer=adam, loss=ssd_loss.compute_loss)</span></pre><p id="44a9" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph=""><strong class="ji jt">Define data loaders</strong></p><pre class="hm hn ho hp hq mn mo du"><span id="4956" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">train_annotation_file=f'<strong class="mp jt">{conf.dataset_folder}</strong>train_pascal.csv'<br>val_annotation_file=f'<strong class="mp jt">{conf.dataset_folder}</strong>val_pascal.csv'<br>subset_annotation_file=f'<strong class="mp jt">{conf.dataset_folder}</strong>small_pascal.csv'</span><span id="3ab2" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">batch_size=4<br>ret_5_elements={'original_images','processed_images','processed_labels','filenames','inverse_transform'}</span><span id="adf7" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">train_generator = learner.get_generator(batch_size, trans=trans, anot_file=train_annotation_file,<br>                  encoder=ssd_input_encoder)</span><span id="14f2" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">val_generator = learner.get_generator(batch_size,trans=trans, anot_file=val_annotation_file,<br>                 returns={'processed_images','encoded_labels'}, encoder=ssd_input_encoder,val=True)</span></pre><h2 id="5f42" class="jz iv cn ar aq iw ka kb kc kd ke kf kg kh ki kj kk" data-selectable-paragraph="">5. Training the model</h2><p id="774e" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph=""><strong class="ji jt">Now</strong> that the model is ready, we’ll set some last training related definitions, and start training</p><pre class="hm hn ho hp hq mn mo du"><span id="babb" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">learner.init_training()</span><span id="76e0" class="jz iv cn ar mp b bj mt mu mv mw mx mr r ms" data-selectable-paragraph="">history = learner.train(train_generator, val_generator, steps=100,epochs=80)</span></pre><p id="fd5a" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">As a bonus, I’ve included the <strong class="ji jt"><em class="mm">training_plot</em></strong> callback in the training script to visualize a random image after every epoch. For example, here is a snapshot of predictions after <strong class="ji jt">sixth</strong> epoch:</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="eu ev na"><div class="hw r bl hx"><div class="nb r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/1_24S3wR8AoFoes6Ddmn3rLw.png" width="283" height="283" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="283" height="283" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/566/1*24S3wR8AoFoes6Ddmn3rLw.png" width="283" height="283" role="presentation"/></noscript></div></div></div></figure><p id="5f7c" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">The SSD_Keras repo handles saving the model after almost each epoch , so you can load later the models simply by changing the <em class="mm">weights_destination_path</em> line to equal the path</p><pre class="hm hn ho hp hq mn mo du"><span id="3c8b" class="jz iv cn ar mp b bj mq mr r ms" data-selectable-paragraph="">weights_destination_path = &lt;path&gt;</span></pre><p id="6a7c" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">If you followed my instructions, you should be able to train the model. The ssd_keras provides some more features, e.g data augmentations, different loaders, and evaluator. I’ve reached &gt;80 mAP after short training.</p><p id="c821" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">How high did you achieve?</p><figure class="hm hn ho hp hq hr eu ev paragraph-image"><div class="la lb bl lc ak"><div class="eu ev nc"><div class="hw r bl hx"><div class="nd r"><div class="qf rz ey t u ht ak bd hu hv"><img class="ey t u ht ak hz ia ib" src="./A gentle introduction to OCR - Towards Data Science_files/0_y84JlUC2sOzql3um" width="961" height="242" role="presentation"></div><img class="bk hs ey t u ht ak ic" width="961" height="242" role="presentation"><noscript><img class="ey t u ht ak" src="https://miro.medium.com/max/1922/0*y84JlUC2sOzql3um" width="961" height="242" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj km kn ko ew eu ev kp kq aq ef" data-selectable-paragraph="">Training for 4X100X60 samples, from tensorboard</figcaption></figure><h1 id="30b2" class="iu iv cn ar aq iw fm ix fo iy iz ja jb jc jd je jf" data-selectable-paragraph="">Summary</h1><p id="5945" class="jg jh cn ar ji b jj jk jl jm jn jo jp jq jr js in fd" data-selectable-paragraph="">In this post, we discussed different challenges and approaches in the OCR field. As many problems in deep learning/computer vision, it has much more to it than seems at first. We have seen many sub tasks of it, and some different approaches to solve it, neither currently serves as a silver bullet. From the other hand, we’ve seen it is not very hard to reach preliminary results, without too much of hassle.</p><p id="214e" class="jg jh cn ar ji b jj ju jl jv jn jw jp jx jr jy in fd" data-selectable-paragraph="">Hope you’ve enjoyed!</p></div></div></section></div></article><div class="qf fc ne s ak nl nj nm" data-test-id="post-sidebar"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="nn n ex"><div class="sh"><div class="no np r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><h2 class="aq iw nq as cn">Towards Data Science</h2></a><div class="nr ns r"><h4 class="aq ef bj as bd nt bc gn nu gp av">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="bf" aria-hidden="true"><button class="cm co gt gu gv gw gx gy bx cz aq b ar as at au da db dc bf dd ca">Follow</button></div></div><div class="nv nw nx n"><div class="n o"><div class="ny r bl"><div class=""><button class="bv nz oa ob oc od oe of gu og gx"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="oh r"><div class="oi"><h4 class="aq ef bj as av"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd">1.7K </button></h4></div></div></div></div><div><div class="hk"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="qf sh ne s nf ng nh ni nj nk"></div><div><div class="oj hr n ex p"><div class="n p"><div class="ac ae af ag ah fi aj ak"><div class="n ok"></div><div class="n o ok"></div><div class="ol r"><ul class="bv bw"><li class="bf ds om on"><a href="https://towardsdatascience.com/tag/machine-learning" class="oo op dd av r mo oq a b eg">Machine Learning</a></li><li class="bf ds om on"><a href="https://towardsdatascience.com/tag/artificial-intelligence" class="oo op dd av r mo oq a b eg">Artificial Intelligence</a></li><li class="bf ds om on"><a href="https://towardsdatascience.com/tag/ocr" class="oo op dd av r mo oq a b eg">Ocr</a></li><li class="bf ds om on"><a href="https://towardsdatascience.com/tag/computer-vision" class="oo op dd av r mo oq a b eg">Computer Vision</a></li><li class="bf ds om on"><a href="https://towardsdatascience.com/tag/deep-learning" class="oo op dd av r mo oq a b eg">Deep Learning</a></li></ul></div><div class="or n gd ab"><div class="n o"><div class="ck r bl"><div class=""><div class="c os dg n o ot bl ou ov ow ox oy oz pa pb pc pd pe pf pg gy"><button class="bv nz oa ob oc od ph of o ic dg n p pi u ht ey t ak gu og gx pj"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="oh r"><div class="oi"><h4 class="aq ef bj as cn"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd">1.7K claps</button></h4></div></div></div><div class="n o"><div class="hi r ap"><a href="https://medium.com/p/ee1469a201aa/share/twitter?source=post_actions_footer---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hi r ap"><a href="https://medium.com/p/ee1469a201aa/share/facebook?source=post_actions_footer---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="hi r ap"><div><div class="hk"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="bf" aria-hidden="true"><div class="bf" aria-hidden="true"><div class="r ap"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="pk pl pm ol r ab"><div class="pn po r bl"><span class="r pp an pq"><div class="r ey pr ps"><a href="https://towardsdatascience.com/@gidishperber?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Gidi Shperber" class="r dg dy pt" src="./A gentle introduction to OCR - Towards Data Science_files/0_F1qzwbKctxWXpQhr(1).jpg" width="80" height="80"></a></div><span class="r"><div class="pu r pv"><p class="aq ef eg as av ei pw">Written by</p></div><div class="pu px n pv"><div class="ak n o gd"><h2 class="aq iw lw py cn"><a href="https://towardsdatascience.com/@gidishperber?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener">Gidi Shperber</a></h2><div class="r g"><button class="cm co gt gu gv gw gx gy bx cz aq b ar as at au da db dc bf dd ca">Follow</button></div></div></div></span></span><div class="pu pz r pv dl"><div class="qa r"><h4 class="aq ef nq qb av">CEO at Shibumi.AI</h4></div><div class="ci qc dl"><button class="cm co gt gu gv gw gx gy bx cz aq b ar as at au da db dc bf dd ca">Follow</button></div></div></div><div class="pk r"></div><div class="pn po r bl"><span class="r pp an pq"><div class="r ey pr ps"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="cz pt dy" src="./A gentle introduction to OCR - Towards Data Science_files/1_hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="pu px n pv"><div class="ak n o gd"><h2 class="aq iw lw py cn"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="bf" aria-hidden="true"><button class="cm co gt gu gv gw gx gy bx cz aq b ar as at au da db dc bf dd ca">Follow</button></div></div></div></div></span></span><div class="pu qd r pv dl"><div class="qa r"><h4 class="aq ef nq qb av">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="ci qc dl"><div class="bf" aria-hidden="true"><button class="cm co gt gu gv gw gx gy bx cz aq b ar as at au da db dc bf dd ca">Follow</button></div></div></div></div></div><div class="qe pl r ab"><a href="https://medium.com/p/ee1469a201aa/responses/show?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><span class="ig qf oc"><div class="mn qg cz r ko dl"><span class="gt">See responses (4)</span></div></span></a></div></div></div><div class="qh r qi ab"><div class="n p"><div class="ac ae af ag ah ai aj ak"></div></div></div></div></div><div class="qj r qk ql"><section class="eu ev ak dc r qm qn qo qp qq qr qs qt qu qv qw qx qy qz ra"><div class="rb rc pn n gd g"><div class="rd n gd"><div class="re r rf"><div class="rg r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx rh ri ca cb rj rk" rel="noopener"><h4 class="rl rm rn aq iw ar qb ro rp r">Discover <!-- -->Medium</h4></a></div><span class="aq b ar as at au r rq rr">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb rj rk rs" rel="noopener">Watch</a></span></div><div class="re r rf"><div class="rt r"><a href="https://medium.com/topics?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx rh ri ca cb rj rk" rel="noopener"><h4 class="rl rm rn aq iw ar qb ro rp r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="aq b ar as at au r rq rr">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb rj rk rs" rel="noopener">Explore</a></span></div><div class="re r rf"><div class="rg r"><a href="https://medium.com/membership?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx rh ri ca cb rj rk" rel="noopener"><h4 class="rl rm rn aq iw ar qb ro rp r">Become a member</h4></a></div><span class="aq b ar as at au r rq rr">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb rj rk rs" rel="noopener">Upgrade</a></span></div></div></div><div class="n o gd"><a href="https://medium.com/?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx rh ri ca cb rj rk" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="rm"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="aq b ar as at au r rq rr"><div class="ru rv n gd rw an"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx gq ca cb rj rk" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx gq ca cb rj rk" rel="noopener">Help</a><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----ee1469a201aa----------------------" class="bo bp bq br bs bt bu bv bw bx gq ca cb rj rk" rel="noopener">Legal</a></div></span></div></section></div></div></div><script src="./A gentle introduction to OCR - Towards Data Science_files/16180790160.js"></script><iframe src="./A gentle introduction to OCR - Towards Data Science_files/a16180790160.html" hidden="" aria-hidden="true" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200207-221127-d173479dd0","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200207-221127-d173479dd0"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[]},"debug":{"requestId":"d7c4421c-581a-4c0d-87e0-9cb557e3cf0f","originalSpanCarrier":{"ot-tracer-spanid":"6fd03e5a0e2e1f23","ot-tracer-traceid":"4460e3dd6e7af1cd","ot-tracer-sampled":"true"}},"session":{"user":{"id":"a2050f27d275"},"xsrf":"riylmAvkKxQC"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hasRenderedBranchBanner":null,"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"branchData":{"loaded":false},"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-gentle-introduction-to-ocr-ee1469a201aa","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}}},"client":{"isBot":false,"isEu":true,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":true},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"disable_go_social","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_geolocation_stripe_standard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.37":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_membership_thank_you_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_more_branch_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_new_pub_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_rito_caching","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_sign_up_with_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.84":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.90":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.92":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.93":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:a2050f27d275","typename":"User"},"meterPost({\"postId\":\"ee1469a201aa\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"ee1469a201aa\"})":{"type":"id","generated":false,"id":"Post:ee1469a201aa","typename":"Post"},"notificationsConnection({})":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({})","typename":"NotificationsConnection"},"notificationStatus":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationStatus","typename":"NotificationStatus"}},"User:a2050f27d275":{"id":"a2050f27d275","username":"johnny.chen_2092","name":"Johnny Chen","imageId":"1*dmbNkD5D-u45r44go_cf0g.png","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"johnny.chen@supelec.fr","unverifiedEmail":"","createdAt":1581065355037,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":4,"unlocksRemaining":4},"Post:ee1469a201aa":{"__typename":"Post","visibility":"PUBLIC","latestPublishedVersion":"7ff608540e21","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"id":"ee1469a201aa","creator":{"type":"id","generated":false,"id":"User:1dbbeb01604b","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","sequence":null,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-gentle-introduction-to-ocr-ee1469a201aa","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1540192584241,"isPublished":true,"layerCake":4,"primaryTopic":null,"title":"A gentle introduction to OCR","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":15.30566037735849,"readingList":"READING_LIST_NONE","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:ocr","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:computer-vision","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"}],"viewerClapCount":null,"clapCount":1778,"voterCount":271,"recommenders":[],"responsesCount":4,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1561452917177,"previewContent":{"type":"id","generated":true,"id":"$Post:ee1469a201aa.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*77Hn7O_KROhi37Gt7Aq67Q.png","typename":"ImageMetadata"},"updatedAt":1561452917433,"topics":[],"seoDescription":"","isSuspended":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","domain":"towardsdatascience.com","slug":"towards-data-science","__typename":"Collection","auroraAlphaEnabled":false,"googleAnalyticsId":"UA-19707169-24","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"}},"User:1dbbeb01604b":{"id":"1dbbeb01604b","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Gidi Shperber","isFollowing":false,"username":"gidishperber","bio":"CEO at Shibumi.AI","imageId":"0*F1qzwbKctxWXpQhr.jpg","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"shgidi"},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Picks","url":"https:\u002F\u002Ftowardsdatascience.com\u002Four-picks\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"More","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmore\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"dc1d","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1":{"name":"3562","startIndex":85,"textLayout":"FLOW","imageLayout":"NONE","backgroundImage":null,"videoLayout":"NO_VIDEO","backgroundVideo":null,"__typename":"Section"},"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:ee1469a201aa.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_97","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_98","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_99","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_100","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_101","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_102","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_103","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_104","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_105","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_106","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_107","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_108","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_109","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_110","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_111","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_112","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_113","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_114","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_115","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_116","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_117","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_118","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_119","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_120","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_121","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_122","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_123","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_124","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_125","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_126","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_127","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_128","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_129","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_130","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_131","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_132","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_133","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_134","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_135","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_136","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_137","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_138","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_139","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_140","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_141","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_142","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_143","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_144","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_145","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_146","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_147","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_148","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_149","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_150","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_151","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_152","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_153","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_154","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_155","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_156","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_157","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_158","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_159","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_160","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_161","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_162","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_163","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_164","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_165","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_166","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_167","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_168","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_169","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_170","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_171","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_172","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:7ff608540e21_173","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:7ff608540e21_0":{"id":"7ff608540e21_0","name":"4502","type":"H3","href":null,"layout":null,"metadata":null,"text":"A gentle introduction to OCR","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_1":{"id":"7ff608540e21_1","name":"93da","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*77Hn7O_KROhi37Gt7Aq67Q.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*77Hn7O_KROhi37Gt7Aq67Q.png":{"id":"1*77Hn7O_KROhi37Gt7Aq67Q.png","originalHeight":362,"originalWidth":448,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_2":{"id":"7ff608540e21_2","name":"4457","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Want to learn more? visit www.Shibumi-ai.com","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_2.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_2.markups.0":{"type":"A","start":26,"end":44,"href":"https:\u002F\u002Fwww.shibumi-ai.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_3":{"id":"7ff608540e21_3","name":"5937","type":"H3","href":null,"layout":null,"metadata":null,"text":"Intro","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_4":{"id":"7ff608540e21_4","name":"c413","type":"P","href":null,"layout":null,"metadata":null,"text":"OCR, or optical character recognition, is one of the earliest addressed computer vision tasks, since in some aspects it does not require deep learning. Therefore there were different OCR implementations even before the deep learning boom in 2012, and some even dated back to 1914 (!).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_4.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_4.markups.0":{"type":"STRONG","start":0,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_5":{"id":"7ff608540e21_5","name":"d0fa","type":"P","href":null,"layout":null,"metadata":null,"text":"This makes many people think the OCR challenge is “solved”, it is no longer challenging. Another belief which comes from similar sources is that OCR does not require deep learning, or in other words, using deep learning for OCR is an overkill.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_5.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_5.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_5.markups.0":{"type":"STRONG","start":50,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_5.markups.1":{"type":"STRONG","start":166,"end":179,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_6":{"id":"7ff608540e21_6","name":"d1fc","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyone who practices computer vision, or machine learning in general, knows that there is no such thing as a solved task, and this case is not different. On the contrary, OCR yields very-good results only on very specific use cases, but in general, it is still considered as challenging.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_7":{"id":"7ff608540e21_7","name":"70df","type":"P","href":null,"layout":null,"metadata":null,"text":"Additionally, it’s true there are good solutions for certain OCR tasks that do not require deep learning. However, to really step forward towards better, more general solutions, deep learning will be mandatory.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_8":{"id":"7ff608540e21_8","name":"f0e8","type":"H4","href":null,"layout":null,"metadata":null,"text":"why do I write about OCR?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_9":{"id":"7ff608540e21_9","name":"5700","type":"P","href":null,"layout":null,"metadata":null,"text":"As many of my works\u002Fwrite-ups, this too started off as project for client. I was requested to solve a specific OCR task.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_10":{"id":"7ff608540e21_10","name":"2874","type":"P","href":null,"layout":null,"metadata":null,"text":"During and after working on this task, I’ve reached some conclusions and insights which I was very eager to share with you. Additionally, after intensively working on such a task, it is hard for me to stop and throw it away, so I keep my research going, and hoping to achieve an even better and more generalized solution.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_11":{"id":"7ff608540e21_11","name":"be65","type":"H4","href":null,"layout":null,"metadata":null,"text":"What you’ll find here","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_12":{"id":"7ff608540e21_12","name":"5c98","type":"P","href":null,"layout":null,"metadata":null,"text":"In this post I will explore some of the strategies, methods and logic used to address different OCR tasks, and will share some useful approaches. In the last part, we will tackle a real world problem with code. This should not be considered as an exhaustive review (unfortunately) since the depth, history and breadth of approaches are too wide for this kind of a blog-post.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_12.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_12.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_12.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_12.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_12.markups.0":{"type":"STRONG","start":40,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_12.markups.1":{"type":"STRONG","start":52,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_12.markups.2":{"type":"STRONG","start":64,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_12.markups.3":{"type":"STRONG","start":181,"end":199,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_13":{"id":"7ff608540e21_13","name":"df3c","type":"P","href":null,"layout":null,"metadata":null,"text":"However, as always, I will not spare you from references to articles, data sets, repositories and other relevant blog-posts.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_14":{"id":"7ff608540e21_14","name":"d197","type":"H3","href":null,"layout":null,"metadata":null,"text":"Types of OCR","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_15":{"id":"7ff608540e21_15","name":"b507","type":"P","href":null,"layout":null,"metadata":null,"text":"As I hinted before, there are more than one meaning for OCR. In its most general meaning, it refers to extracting text from every possible image, be it a standard printed page from a book, or a random image with graffiti in it (“in the wild”). In between, you may find many other tasks, such as reading license plates, no-robot captchas, street signs etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_15.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_15.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_15.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_15.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_15.markups.0":{"type":"STRONG","start":229,"end":240,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_15.markups.1":{"type":"STRONG","start":303,"end":317,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_15.markups.2":{"type":"STRONG","start":328,"end":336,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_15.markups.3":{"type":"STRONG","start":338,"end":350,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_16":{"id":"7ff608540e21_16","name":"a216","type":"P","href":null,"layout":null,"metadata":null,"text":"Although each of these options has its own difficulties, clearly “in the wild” task is the hardest.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_17":{"id":"7ff608540e21_17","name":"80f0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*n6BoysCl4xSmMHcDwqK_ZA.png","typename":"ImageMetadata"},"text":"Left: Printed text. Right: text in the wild","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*n6BoysCl4xSmMHcDwqK_ZA.png":{"id":"1*n6BoysCl4xSmMHcDwqK_ZA.png","originalHeight":316,"originalWidth":680,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_18":{"id":"7ff608540e21_18","name":"de5f","type":"P","href":null,"layout":null,"metadata":null,"text":"Form these examples we can draw out some attributes of the OCR tasks:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_18.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_18.markups.0":{"type":"STRONG","start":41,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_19":{"id":"7ff608540e21_19","name":"a33e","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Text density: on a printed\u002Fwritten page, text is dense. However, given an image of a street with a single street sign, text is sparse.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_19.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_19.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_20":{"id":"7ff608540e21_20","name":"bae1","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Structure of text: text on a page is structured, mostly in strict rows, while text in the wild may be sprinkled everywhere, in different rotations.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_20.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_20.markups.0":{"type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_21":{"id":"7ff608540e21_21","name":"8327","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Fonts: printed fonts are easier, since they are more structured then the noisy hand-written characters.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_21.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_21.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_22":{"id":"7ff608540e21_22","name":"85ba","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Character type: text may come in different language which may be very different from each other. Additionally, structure of text may be different from numbers, such as house numbers etc.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_22.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_22.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_23":{"id":"7ff608540e21_23","name":"1908","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Artifacts: clearly, outdoor pictures are much noisier than the comfortable scanner.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_23.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_23.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_24":{"id":"7ff608540e21_24","name":"77fd","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Location: some tasks include cropped\u002Fcentered text, while in others, text may be located in random locations in the image.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_24.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_24.markups.0":{"type":"STRONG","start":0,"end":8,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_25":{"id":"7ff608540e21_25","name":"aa2c","type":"H3","href":null,"layout":null,"metadata":null,"text":"Data sets\u002FTasks","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_26":{"id":"7ff608540e21_26","name":"7332","type":"H4","href":null,"layout":null,"metadata":null,"text":"SVHN","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_27":{"id":"7ff608540e21_27","name":"8bb0","type":"P","href":null,"layout":null,"metadata":null,"text":"A good place to start from is SVHN, Street View House Numbers data-set. As its name implies, this is a data-set of house numbers extracted from google street view. The task difficulty is intermediate. The digits come in various shapes and writing styles, however, each house number is located in the middle of the image, thus detection is not required. The images are not of a very high resolution, and their arrangement may be a bit peculiar.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_27.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_27.markups.0":{"type":"A","start":30,"end":34,"href":"http:\u002F\u002Fufldl.stanford.edu\u002Fhousenumbers\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_28":{"id":"7ff608540e21_28","name":"c80f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*wa6uLBFTeg9rcSoC","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*wa6uLBFTeg9rcSoC":{"id":"0*wa6uLBFTeg9rcSoC","originalHeight":728,"originalWidth":1091,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_29":{"id":"7ff608540e21_29","name":"88a3","type":"H4","href":null,"layout":null,"metadata":null,"text":"License plates","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_30":{"id":"7ff608540e21_30","name":"bed6","type":"P","href":null,"layout":null,"metadata":null,"text":"Another common challenge, which is not very hard and useful in practice, is the license plate recognition. This task, as most OCR tasks, requires to detect the license plate, and then recognizing it’s characters. Since the plate’s shape is relatively constant, some approach use simple reshaping method before actually recognizing the digits. Here are some examples from the web:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_30.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_30.markups.0":{"type":"STRONG","start":201,"end":211,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_31":{"id":"7ff608540e21_31","name":"f86d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*3cGk3lVSAiaRAnPCqDCEEw.png","typename":"ImageMetadata"},"text":"OpenALPR example. with car type a s abonus","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*3cGk3lVSAiaRAnPCqDCEEw.png":{"id":"1*3cGk3lVSAiaRAnPCqDCEEw.png","originalHeight":253,"originalWidth":782,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_32":{"id":"7ff608540e21_32","name":"6a75","type":"OLI","href":null,"layout":null,"metadata":null,"text":"OpenALPR is a very robust tool, with no deep learning involved, to recognize license plates from various countries","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_32.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_32.markups.0":{"type":"A","start":0,"end":8,"href":"https:\u002F\u002Fgithub.com\u002Fopenalpr\u002Fopenalpr","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_33":{"id":"7ff608540e21_33","name":"fb5f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"This repo provides an implementation of CRNN model (will be further discussed) to recognize Korean license plates.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_33.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_33.markups.0":{"type":"A","start":5,"end":9,"href":"https:\u002F\u002Fgithub.com\u002Fqjadud1994\u002FCRNN-Keras","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_34":{"id":"7ff608540e21_34","name":"9488","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Supervise.ly, a data utilities company, wrote about training a license plate recognizer using artificial data generated by their tool (artificial data will also be further discussed)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_34.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_34.markups.0":{"type":"A","start":40,"end":45,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fnumber-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_35":{"id":"7ff608540e21_35","name":"3195","type":"H4","href":null,"layout":null,"metadata":null,"text":"CAPTCHA","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_36":{"id":"7ff608540e21_36","name":"7009","type":"P","href":null,"layout":null,"metadata":null,"text":"Since the internet is full of robots, a common practice to tell them apart from real humans, are vision tasks, specifically text reading, aka CAPTCHA. Many of these texts are random and distorted, which should make it harder for computer to read. I’m not sure whoever developed the CAPTCHA predicted the advances in computer vision, however most of today text CAPTCHAs are not very hard to solve, especially if we don’t try to solve all of them at once.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_37":{"id":"7ff608540e21_37","name":"ebab","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*sRzszQ-SBtcGW3KyQwJtEQ.png","typename":"ImageMetadata"},"text":"Facebook knows how to make challenging CAPTCHAs","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*sRzszQ-SBtcGW3KyQwJtEQ.png":{"id":"1*sRzszQ-SBtcGW3KyQwJtEQ.png","originalHeight":253,"originalWidth":472,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_38":{"id":"7ff608540e21_38","name":"25a7","type":"P","href":null,"layout":null,"metadata":null,"text":"Adam Geitgey provides a nice tutorial to solving some CAPTCHAs with deep learning, which includes synthesizing artificial data once again.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_38.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_38.markups.0":{"type":"A","start":24,"end":37,"href":"https:\u002F\u002Fmedium.com\u002F@ageitgey\u002Fhow-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_39":{"id":"7ff608540e21_39","name":"1181","type":"H4","href":null,"layout":null,"metadata":null,"text":"PDF OCR","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_40":{"id":"7ff608540e21_40","name":"a370","type":"P","href":null,"layout":null,"metadata":null,"text":"The most common scenario for OCR is the printed\u002Fpdf OCR. The structured nature of printed documents make it much easier to parse them. Most OCR tools (e.g Tesseract) are mostly intended to address this task, and achieve good result. Therefore, I will not elaborate too much on this task in this post.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_40.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_40.markups.0":{"type":"A","start":155,"end":164,"href":"https:\u002F\u002Fgithub.com\u002Ftesseract-ocr\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_41":{"id":"7ff608540e21_41","name":"eb27","type":"H4","href":null,"layout":null,"metadata":null,"text":"OCR in the wild","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_42":{"id":"7ff608540e21_42","name":"2fc0","type":"P","href":null,"layout":null,"metadata":null,"text":"This is the most challenging OCR task, as it introduces all general computer vision challenges such as noise, lighting, and artifacts into OCR. Some relevant data-sets for this task is the coco-text, and the SVT data set which once again, uses street view images to extract text from.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_42.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_42.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_42.markups.0":{"type":"A","start":189,"end":198,"href":"https:\u002F\u002Fvision.cornell.edu\u002Fse3\u002Fcoco-text-2\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_42.markups.1":{"type":"A","start":208,"end":211,"href":"http:\u002F\u002Ftc11.cvc.uab.es\u002Fdatasets\u002FSVT_1","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_43":{"id":"7ff608540e21_43","name":"5fc2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*h1CDmAxHf1Ee0illDor2HA.png","typename":"ImageMetadata"},"text":"COCO text example","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*h1CDmAxHf1Ee0illDor2HA.png":{"id":"1*h1CDmAxHf1Ee0illDor2HA.png","originalHeight":369,"originalWidth":526,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_44":{"id":"7ff608540e21_44","name":"06b0","type":"H4","href":null,"layout":null,"metadata":null,"text":"Synth text","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_45":{"id":"7ff608540e21_45","name":"97de","type":"P","href":null,"layout":null,"metadata":null,"text":"SynthText is not a data-set, and perhaps not even a task, but a nice idea to improve training efficiency is artificial data generation. Throwing random characters or words on an image will seem much more natural than any other object, because of the flat nature of text.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_45.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_45.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_46":{"id":"7ff608540e21_46","name":"9a53","type":"P","href":null,"layout":null,"metadata":null,"text":"We have seen earlier some data generation for easier tasks like CAPTCHA and license plate. Generating text in the wild is a little bit more complex. The task includes considering depth information of an image. Fortunately, SynthText is a nice work that takes in images with the aforementioned annotations, and intelligently sprinkles words (from newsgroup data-set).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_47":{"id":"7ff608540e21_47","name":"7f1d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GLFRyZ99LJHmCewUqhKLdA.png","typename":"ImageMetadata"},"text":"SynthText process illustration: top right is the segmentation of an image, bottom right is the depth data. Bottom left is a surface analyses of the image, which according to text is sprinkled on the image.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GLFRyZ99LJHmCewUqhKLdA.png":{"id":"1*GLFRyZ99LJHmCewUqhKLdA.png","originalHeight":530,"originalWidth":633,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_48":{"id":"7ff608540e21_48","name":"8156","type":"P","href":null,"layout":null,"metadata":null,"text":"To make the “sprinkled” text look realistic and useful, the SynthText library takes with every image two masks, one of depth and another of segmentation. If you like to use your own images, you should add this data as well","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_49":{"id":"7ff608540e21_49","name":"b99b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"It is recommended to check the repo and generate some images on your own. You should pay attention that the repo uses some outdated version of opencv and maptlotlib, so some modifications may be necessary.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_49.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_49.markups.0":{"type":"A","start":31,"end":35,"href":"https:\u002F\u002Fgithub.com\u002Fankush-me\u002FSynthText","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_50":{"id":"7ff608540e21_50","name":"2c34","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*xs7V0gNwOdgfJkP6","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*xs7V0gNwOdgfJkP6":{"id":"0*xs7V0gNwOdgfJkP6","originalHeight":328,"originalWidth":468,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_51":{"id":"7ff608540e21_51","name":"3abd","type":"H4","href":null,"layout":null,"metadata":null,"text":"Mnist","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_52":{"id":"7ff608540e21_52","name":"7be3","type":"P","href":null,"layout":null,"metadata":null,"text":"Although not really an OCR task, it is impossible to write about OCR and not include the Mnist example. The most well known computer vision challenge is not really an considered and OCR task, since it contains one character (digit) at a time, and only 10 digits. However, it may hint why OCR is considered easy. Additionally, in some approaches every letter will be detected separately, and then Mnist like (classification) models become relevantץ","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_53":{"id":"7ff608540e21_53","name":"1348","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*JdoUIidwCrrUZPPc","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*JdoUIidwCrrUZPPc":{"id":"0*JdoUIidwCrrUZPPc","originalHeight":49,"originalWidth":534,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_54":{"id":"7ff608540e21_54","name":"4dd0","type":"H3","href":null,"layout":null,"metadata":null,"text":"Strategies","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_55":{"id":"7ff608540e21_55","name":"dc14","type":"P","href":null,"layout":null,"metadata":null,"text":"As we’ve seen and implied, the text recognition is mostly a two-step task. First, you would like to detect the text(s) appearances in the image, may it be dense (as in printed document) or sparse (As text in the wild).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_55.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_55.markups.0":{"type":"STRONG","start":100,"end":106,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_56":{"id":"7ff608540e21_56","name":"e06b","type":"P","href":null,"layout":null,"metadata":null,"text":"After detecting the line\u002Fword level we can choose once again from a large set of solutions, which generally come from three main approaches:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_57":{"id":"7ff608540e21_57","name":"ce04","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Classic computer vision techniques.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_58":{"id":"7ff608540e21_58","name":"229a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Specialized deep learning.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_59":{"id":"7ff608540e21_59","name":"f73f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Standard deep learning approach (Detection).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_60":{"id":"7ff608540e21_60","name":"94e1","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s examine each of them:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_61":{"id":"7ff608540e21_61","name":"4016","type":"H4","href":null,"layout":null,"metadata":null,"text":"1. Classic computer vision techniques","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_62":{"id":"7ff608540e21_62","name":"e187","type":"P","href":null,"layout":null,"metadata":null,"text":"As said earlier, computer vision solves various text recognition problems for a long time. You can find many examples online:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_63":{"id":"7ff608540e21_63","name":"e1ae","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The Great Adrian Rosebrook has a tremendous number of tutorials in his site, like this one, this one and more.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_63.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_63.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_63.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_63.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_63.markups.0":{"type":"A","start":87,"end":90,"href":"https:\u002F\u002Fwww.pyimagesearch.com\u002F2017\u002F07\u002F17\u002Fcredit-card-ocr-with-opencv-and-python\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_63.markups.1":{"type":"A","start":97,"end":100,"href":"https:\u002F\u002Fwww.pyimagesearch.com\u002F2017\u002F07\u002F24\u002Fbank-check-ocr-with-opencv-and-python-part-i\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_63.markups.2":{"type":"A","start":105,"end":109,"href":"https:\u002F\u002Fwww.pyimagesearch.com\u002Fcategory\u002Foptical-character-recognition-ocr\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_63.markups.3":{"type":"STRONG","start":10,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_64":{"id":"7ff608540e21_64","name":"5bf5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Stack overflow has also some gems like this one.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_64.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_64.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_64.markups.0":{"type":"A","start":39,"end":43,"href":"https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F9413216\u002Fsimple-digit-recognition-ocr-in-opencv-python","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_64.markups.1":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_65":{"id":"7ff608540e21_65","name":"8c06","type":"P","href":null,"layout":null,"metadata":null,"text":"The classic-CV approach generally claims:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_66":{"id":"7ff608540e21_66","name":"1d33","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Apply filters to make the characters stand out from the background.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_66.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_66.markups.0":{"type":"STRONG","start":6,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_67":{"id":"7ff608540e21_67","name":"0ff0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Apply contour detection to recognize the characters one by one.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_67.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_67.markups.0":{"type":"STRONG","start":6,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_68":{"id":"7ff608540e21_68","name":"bbda","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Apply image classification to identify the characters","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_68.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_68.markups.0":{"type":"STRONG","start":6,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_69":{"id":"7ff608540e21_69","name":"2b5b","type":"P","href":null,"layout":null,"metadata":null,"text":"Clearly, if part two is done well, part three is easy either with pattern matching or machine learning (e.g Mnist).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_70":{"id":"7ff608540e21_70","name":"e4ce","type":"P","href":null,"layout":null,"metadata":null,"text":"However, the contour detection is quite challenging for generalization. it requires a lot of manual fine tuning, therefore becomes infeasible in most of the problem. e.g, lets apply a simple computer vision script from here on some images from SVHN data-set. At first attempt we may achieve very good results:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_70.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_70.markups.0":{"type":"A","start":219,"end":223,"href":"http:\u002F\u002Fscikit-image.org\u002Fdocs\u002Fdev\u002Fauto_examples\u002Fsegmentation\u002Fplot_label.html#sphx-glr-download-auto-examples-segmentation-plot-label-py","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_71":{"id":"7ff608540e21_71","name":"6af2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*R6r8eGNyoylNVj4G","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*R6r8eGNyoylNVj4G":{"id":"0*R6r8eGNyoylNVj4G","originalHeight":252,"originalWidth":299,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_72":{"id":"7ff608540e21_72","name":"5fa4","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*PMmcCq_CtcwIYg0x","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*PMmcCq_CtcwIYg0x":{"id":"0*PMmcCq_CtcwIYg0x","originalHeight":236,"originalWidth":280,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_73":{"id":"7ff608540e21_73","name":"7dca","type":"P","href":null,"layout":null,"metadata":null,"text":"But when characters are closer to each other, things start to break:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_74":{"id":"7ff608540e21_74","name":"6614","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*HwWJowf1UobFjbNR","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*HwWJowf1UobFjbNR":{"id":"0*HwWJowf1UobFjbNR","originalHeight":252,"originalWidth":299,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_75":{"id":"7ff608540e21_75","name":"a6cd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*lVny6F7mu7wzhFAG","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*lVny6F7mu7wzhFAG":{"id":"0*lVny6F7mu7wzhFAG","originalHeight":236,"originalWidth":280,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_76":{"id":"7ff608540e21_76","name":"0db6","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ve found out the hard way, that when you start messing around with the parameters, you may reduce such errors, but unfortunately cause others. In other words, if your task is not straightforward, these methods are not the way to go.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_77":{"id":"7ff608540e21_77","name":"940f","type":"H4","href":null,"layout":null,"metadata":null,"text":"2. Specialized deep learning approaches","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_78":{"id":"7ff608540e21_78","name":"2cba","type":"P","href":null,"layout":null,"metadata":null,"text":"Most successful deep learning approaches excel in their generality. However, considering the attributes described above, Specialized networks can be very useful.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_79":{"id":"7ff608540e21_79","name":"1189","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ll examine here an unexhaustive sample of some prominent approaches, and will do a very quick summary of the articles which present them. As always, every article is opened with the words “task X (text recognition) gains attention lately” and goes on to describe their method in detail. Reading the articles carefully will reveal these methods are assembled from pieces of previous deep learning\u002Ftext recognition works.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_80":{"id":"7ff608540e21_80","name":"32dd","type":"P","href":null,"layout":null,"metadata":null,"text":"Results are also depicted thoroughly, however due to many differences in design (including minor differences in data sets)actual comparison is quite impossible. The only way to actually know the performance of these methods on your task, is to get their code (best to worse: find official repo, find unofficial but highly rated repo, implement by yourself) and try it on your data.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_80.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_80.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_80.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_80.markups.0":{"type":"STRONG","start":280,"end":288,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_80.markups.1":{"type":"STRONG","start":300,"end":327,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_80.markups.2":{"type":"STRONG","start":334,"end":343,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_81":{"id":"7ff608540e21_81","name":"42ba","type":"P","href":null,"layout":null,"metadata":null,"text":"Thus, we will always prefer articles with good accompanying repos, and even demos if possible.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_82":{"id":"7ff608540e21_82","name":"b9cd","type":"P","href":null,"layout":null,"metadata":null,"text":"EAST","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_82.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_82.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_83":{"id":"7ff608540e21_83","name":"583c","type":"P","href":null,"layout":null,"metadata":null,"text":"EAST ( Efficient accurate scene text detector) is a simple yet powerful approach for text detection. Using a specialized network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_83.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_83.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_83.markups.0":{"type":"A","start":0,"end":4,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1704.03155.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_83.markups.1":{"type":"STRONG","start":85,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_84":{"id":"7ff608540e21_84","name":"5de3","type":"P","href":null,"layout":null,"metadata":null,"text":"Unlike the other methods we’ll discuss, is limited only to text detection (not actual recognition) however it’s robustness make it worth mentioning.\n Another advantage is that it was also added to open-CV library (from version 4) so you can easily use it (see tutorial here).\n The network is actually a version of the well known U-Net, which good for detecting features that may vary in size. The underlying feed forward “stem” (as coined in the article, see figure below) of this network may very — PVANet is used in the paper, however opencv implementation use Resnet. Obviously, it can be also pre-trained (with imagenet e.g) . As in U-Net, features are extracted from different levels in the network.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_84.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_84.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_84.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_84.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_84.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_84.markups.0":{"type":"A","start":269,"end":273,"href":"https:\u002F\u002Fwww.pyimagesearch.com\u002F2018\u002F08\u002F20\u002Fopencv-text-detection-east-text-detector\u002F","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_84.markups.1":{"type":"STRONG","start":197,"end":205,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_84.markups.2":{"type":"STRONG","start":329,"end":334,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_84.markups.3":{"type":"STRONG","start":500,"end":506,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_84.markups.4":{"type":"STRONG","start":563,"end":569,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_85":{"id":"7ff608540e21_85","name":"3e38","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Kd5tAQp83XY2udt8BE1d0A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Kd5tAQp83XY2udt8BE1d0A.png":{"id":"1*Kd5tAQp83XY2udt8BE1d0A.png","originalHeight":383,"originalWidth":363,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_86":{"id":"7ff608540e21_86","name":"749d","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, the network allows two types of outputs rotated bounding boxes: either a standard bounding box with a rotation angle (2X2+1 parameters) or “quadrangle” which is merely a rotated bounding box with coordinates of all vertices.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_87":{"id":"7ff608540e21_87","name":"ec4c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*BVae8WaaVsznWP9cVHJotw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BVae8WaaVsznWP9cVHJotw.png":{"id":"1*BVae8WaaVsznWP9cVHJotw.png","originalHeight":177,"originalWidth":458,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_88":{"id":"7ff608540e21_88","name":"8a03","type":"P","href":null,"layout":null,"metadata":null,"text":"If real life results will be as in the above images, recognizing the texts will not take much of an effort. However, real life results are not perfect.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_89":{"id":"7ff608540e21_89","name":"d370","type":"P","href":null,"layout":null,"metadata":null,"text":"CRNN","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_89.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_89.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_90":{"id":"7ff608540e21_90","name":"9bfc","type":"P","href":null,"layout":null,"metadata":null,"text":"Convolutional-recurrent neural network, is an article from 2015, which suggest a hybrid (or tribrid?) end to end architecture, that is intended to capture words, in a three step approach.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_91":{"id":"7ff608540e21_91","name":"c34f","type":"P","href":null,"layout":null,"metadata":null,"text":"The idea goes as follows: the first level is a standard fully convolutional network. The last layer of the net is defined as feature layer, and divided into “feature columns”. See in the image below how every such feature column is intended to represent a certain section in the text.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_92":{"id":"7ff608540e21_92","name":"5794","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*5QhwZ7bChMaTsI8ZRvCY_w.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*5QhwZ7bChMaTsI8ZRvCY_w.png":{"id":"1*5QhwZ7bChMaTsI8ZRvCY_w.png","originalHeight":244,"originalWidth":341,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_93":{"id":"7ff608540e21_93","name":"234d","type":"P","href":null,"layout":null,"metadata":null,"text":"Afterwards, the feature columns are fed into a deep-bidirectional LSTM which outputs a sequence, and is intended for finding relations between the characters.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_94":{"id":"7ff608540e21_94","name":"d7c0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*nGWtig3Cd0Jma2nX","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*nGWtig3Cd0Jma2nX":{"id":"0*nGWtig3Cd0Jma2nX","originalHeight":524,"originalWidth":447,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_95":{"id":"7ff608540e21_95","name":"ef48","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, the third part is a transcription layer. Its goal is to take the messy character sequence, in which some characters are redundant and others are blank, and use probabilistic method to unify and make sense out of it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_96":{"id":"7ff608540e21_96","name":"e6d8","type":"P","href":null,"layout":null,"metadata":null,"text":"This method is called CTC loss, and can be read about here. This layer can be used with\u002Fwithout predefined lexicon, which may facilitate predictions of words.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_96.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_96.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_96.markups.0":{"type":"A","start":54,"end":58,"href":"https:\u002F\u002Fgab41.lab41.org\u002Fspeech-recognition-you-down-with-ctc-8d3b558943f0","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_96.markups.1":{"type":"STRONG","start":22,"end":30,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_97":{"id":"7ff608540e21_97","name":"8cd4","type":"P","href":null,"layout":null,"metadata":null,"text":"This paper reaches high (\u003E95%) rates of accuracy with fixed text lexicon, and varying rates of success without it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_98":{"id":"7ff608540e21_98","name":"7ccd","type":"H4","href":null,"layout":null,"metadata":null,"text":"STN-net\u002FSEE","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_99":{"id":"7ff608540e21_99","name":"66ff","type":"P","href":null,"layout":null,"metadata":null,"text":"SEE — Semi-Supervised End-to-End Scene Text Recognition, is a work by Christian Bartzi. He and his colleagues apply a truly end to end strategy to detect and recognize text. They use very weak supervision (which they refer to as semi-supervision, in a different meaning than usual ). as they train the network with only text annotation (without bounding boxes). This allows them use more data, but makes their training procedure quite challenging, and they discuss different tricks to make it work, e.g not training on images with more than two lines of text (at least at the first stages of training).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_99.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_99.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_99.markups.0":{"type":"A","start":0,"end":3,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1712.05404.pdf","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_99.markups.1":{"type":"STRONG","start":315,"end":335,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_100":{"id":"7ff608540e21_100","name":"2a32","type":"P","href":null,"layout":null,"metadata":null,"text":"The paper has an earlier version which is called STN OCR. In the final paper the researchers have refined their methods and presentation, and additionally they’ve put more emphasis on the generality of their approach on the account of high quality of the results.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_100.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_100.markups.0":{"type":"A","start":49,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1707.08831","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_101":{"id":"7ff608540e21_101","name":"9a1f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*XAUtH9C1iPLa9clk9RA-8Q.png","typename":"ImageMetadata"},"text":"SEE strategy","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XAUtH9C1iPLa9clk9RA-8Q.png":{"id":"1*XAUtH9C1iPLa9clk9RA-8Q.png","originalHeight":239,"originalWidth":782,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_102":{"id":"7ff608540e21_102","name":"bb07","type":"P","href":null,"layout":null,"metadata":null,"text":"The name STN-OCR hints on the strategy, of using spatial transformer (=STN, no relation to the recent google transformer).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_102.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_102.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_102.markups.0":{"type":"A","start":49,"end":68,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02025.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_102.markups.1":{"type":"STRONG","start":9,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_103":{"id":"7ff608540e21_103","name":"7178","type":"P","href":null,"layout":null,"metadata":null,"text":"They train two concatenated networks in which the first network, the transformer, learns a transformation on the image to output an easier sub-image to interpret.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_103.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_103.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_103.markups.0":{"type":"STRONG","start":11,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_103.markups.1":{"type":"STRONG","start":39,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_104":{"id":"7ff608540e21_104","name":"7b4c","type":"P","href":null,"layout":null,"metadata":null,"text":"Then, another feed forward network with LSTM on top (hmm… seems like we’ve seen it before) to recognize the text.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_105":{"id":"7ff608540e21_105","name":"745b","type":"P","href":null,"layout":null,"metadata":null,"text":"The researches emphasize here the importance of using resnet(they use it twice) since it provides “strong” propagation to the early layers. however this practice quite accepted nowadays.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_106":{"id":"7ff608540e21_106","name":"5c90","type":"P","href":null,"layout":null,"metadata":null,"text":"Either way, this is an interesting approach to try.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_107":{"id":"7ff608540e21_107","name":"ccf6","type":"H4","href":null,"layout":null,"metadata":null,"text":"3. Standard deep learning approach","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_108":{"id":"7ff608540e21_108","name":"1500","type":"P","href":null,"layout":null,"metadata":null,"text":"As the header implies, after detecting the “words” we can apply standard deep learning detection approaches, such as SSD, YOLO and Mask RCNN. I’m not going to elaborate too much on theses approaches since there is a plethora of info online.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_109":{"id":"7ff608540e21_109","name":"8647","type":"P","href":null,"layout":null,"metadata":null,"text":"I must say this is currently my favorite approach, since what I like in deep learning is the “end to end” philosophy, where you apply a strong model which with some tuning will solve almost every problem. In the next section of this post we will see how it actually works.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_110":{"id":"7ff608540e21_110","name":"c122","type":"P","href":null,"layout":null,"metadata":null,"text":"However, SSD and other detection models are challenged when it comes to dense, similar classes, as reviewed here. I find it a bit ironic since in fact, deep learning models find it much more difficult to recognize digits and letters than to recognize much more challenging and elaborate objects such as dogs, cats or humans. They tend no to reach the desired accuracy, and therefore, specialized approaches thrive.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_110.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_110.markups.0":{"type":"A","start":108,"end":112,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.10012.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_111":{"id":"7ff608540e21_111","name":"0411","type":"H3","href":null,"layout":null,"metadata":null,"text":"Practical Example","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_112":{"id":"7ff608540e21_112","name":"8670","type":"P","href":null,"layout":null,"metadata":null,"text":"So after all the talking, it’s time to get our hands dirty, and try some modelling ourselves. We will try the tackle SVHN task. The SVHN data contains three different data-sets: train, test and extra. The differences are not 100% clear, however the extra data-set which is the biggest (with ~500K samples) includes images that are somehow easier to recognize. So for the sake of this take we will use it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_112.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_112.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_112.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_112.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_112.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_112.markups.0":{"type":"A","start":117,"end":121,"href":"http:\u002F\u002Fufldl.stanford.edu\u002Fhousenumbers\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_112.markups.1":{"type":"EM","start":178,"end":183,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_112.markups.2":{"type":"EM","start":185,"end":189,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_112.markups.3":{"type":"EM","start":194,"end":199,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_112.markups.4":{"type":"EM","start":249,"end":254,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_113":{"id":"7ff608540e21_113","name":"d127","type":"P","href":null,"layout":null,"metadata":null,"text":"To prepare for the task, do the following:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_114":{"id":"7ff608540e21_114","name":"2a8f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"You’ll need a basic GPU machine with Tensorflow≥1.4 and Keras≥2","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_115":{"id":"7ff608540e21_115","name":"6cc3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Clone the SSD_Keras project from here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_115.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_115.markups.0":{"type":"A","start":33,"end":37,"href":"https:\u002F\u002Fgithub.com\u002Fpierluigiferrari\u002Fssd_keras","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_116":{"id":"7ff608540e21_116","name":"a494","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Download the pre-trained SSD300 model on coco data-set from here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_116.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_116.markups.0":{"type":"A","start":60,"end":64,"href":"https:\u002F\u002Fdrive.google.com\u002Fopen?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_117":{"id":"7ff608540e21_117","name":"785b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Clone this project's repo from here..","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_117.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_117.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_117.markups.0":{"type":"A","start":31,"end":35,"href":"https:\u002F\u002Fgithub.com\u002Fshgidi\u002FOCR","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_117.markups.1":{"type":"EM","start":6,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_118":{"id":"7ff608540e21_118","name":"d9cc","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Download the extra.tar.gz file, which contains the extra images of SVHN data-set.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_118.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_118.markups.0":{"type":"A","start":13,"end":25,"href":"http:\u002F\u002Fufldl.stanford.edu\u002Fhousenumbers\u002Fextra.tar.gz","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_119":{"id":"7ff608540e21_119","name":"9d2b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Update all relevant paths in json_config.json in this project repo.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_120":{"id":"7ff608540e21_120","name":"a7d3","type":"P","href":null,"layout":null,"metadata":null,"text":"To efficiently follow the process, you should read the below instruction along with running the ssd_OCR.ipynb notebook from the project’s repo.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_120.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_120.markups.0":{"type":"STRONG","start":96,"end":109,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_121":{"id":"7ff608540e21_121","name":"a7a6","type":"P","href":null,"layout":null,"metadata":null,"text":"And… You are ready to start!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_122":{"id":"7ff608540e21_122","name":"31c1","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: parse the data","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_123":{"id":"7ff608540e21_123","name":"a1a7","type":"P","href":null,"layout":null,"metadata":null,"text":"Like it or not, but there is no “golden” format for data representation in detection tasks. Some well known formats are: coco, via, pascal, xml. And there are more. For instance, the SVHN data-set is annotated with the obscure .mat format. Fortunately for us, this gist provides a slick read_process_h5 script to convert the .mat file to standard json, and you should go one step ahead and convert it further to pascal format, like so:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_123.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_123.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_123.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_123.markups.0":{"type":"A","start":265,"end":269,"href":"https:\u002F\u002Fgist.github.com\u002Fveeresht\u002F7bf499ee6d81938f8bbdb3c6ef1855bf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_123.markups.1":{"type":"EM","start":227,"end":232,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_123.markups.2":{"type":"EM","start":287,"end":302,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_124":{"id":"7ff608540e21_124","name":"9d2c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def json_to_pascal(json, filename): #filename is the .mat file\n    # convert json to pascal and save as csv\n    pascal_list = []\n    for i in json:\n        for j in range(len(i['labels'])):\n            pascal_list.append({'fname': i['filename'] \n            ,'xmin': int(i['left'][j]), 'xmax': int(i['left'][j]+i['width'][j])\n            ,'ymin': int(i['top'][j]),  'ymax': int(i['top'][j]+i['height'][j])\n            ,'class_id': int(i['labels'][j])})\n    df_pascal = pd.DataFrame(pascal_list,dtype='str')\n    df_pascal.to_csv(filename,index=False)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_125":{"id":"7ff608540e21_125","name":"3c81","type":"PRE","href":null,"layout":null,"metadata":null,"text":"p = read_process_h5(file_path)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_125.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_125.markups.0":{"type":"EM","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_126":{"id":"7ff608540e21_126","name":"e507","type":"PRE","href":null,"layout":null,"metadata":null,"text":"json_to_pascal(p, data_folder+'pascal.csv')","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_127":{"id":"7ff608540e21_127","name":"03b3","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we should have a pascal.csv file that is much more standard and will allow us to progress. If the conversion is to slow, you should take note that we don’t need t all the data samples. ~10K will be enough.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_127.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_127.markups.0":{"type":"EM","start":21,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_128":{"id":"7ff608540e21_128","name":"00eb","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: look at the data","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_129":{"id":"7ff608540e21_129","name":"60a3","type":"P","href":null,"layout":null,"metadata":null,"text":"Before starting the modeling process, you should better do some exploration of the data. I only provide a quick function for sanity test, but I recommend you to do some further analysis:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_130":{"id":"7ff608540e21_130","name":"5a11","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def viz_random_image(df):\n    file = np.random.choice(df.fname)\n    im = skimage.io.imread(data_folder+file)\n    annots =  df[df.fname==file].iterrows()","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_131":{"id":"7ff608540e21_131","name":"c064","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    plt.figure(figsize=(6,6))\n    plt.imshow(im)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_132":{"id":"7ff608540e21_132","name":"add4","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    current_axis = plt.gca()","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_133":{"id":"7ff608540e21_133","name":"6e5b","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    for box in annots:\n        label = box[1]['class_id']\n        current_axis.add_patch(plt.Rectangle(\n            (box[1]['xmin'], box[1]['ymin']), box[1]['xmax']-box[1]['xmin'],\n            box[1]['ymax']-box[1]['ymin'], color='blue', fill=False, linewidth=2))  \n        current_axis.text(box[1]['xmin'], box[1]['ymin'], label, size='x-large', color='white', bbox={'facecolor':'blue', 'alpha':1.0})\n        plt.show()\n","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_134":{"id":"7ff608540e21_134","name":"9ba3","type":"PRE","href":null,"layout":null,"metadata":null,"text":"viz_random_image(df)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_135":{"id":"7ff608540e21_135","name":"a9ae","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*zLCfj1HVgil2JRywTQgjew.png","typename":"ImageMetadata"},"text":"A representative sample form SVHN dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zLCfj1HVgil2JRywTQgjew.png":{"id":"1*zLCfj1HVgil2JRywTQgjew.png","originalHeight":139,"originalWidth":297,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_136":{"id":"7ff608540e21_136","name":"f0aa","type":"P","href":null,"layout":null,"metadata":null,"text":"For the following steps, I provide a utils_ssd.py in the repo that facilitates the training, weight loading etc. Some of the code is taken from the SSD_Keras repo, which is also used extensively.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_136.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_136.markups.0":{"type":"EM","start":37,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_137":{"id":"7ff608540e21_137","name":"c228","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3: choosing strategy","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_138":{"id":"7ff608540e21_138","name":"29cd","type":"P","href":null,"layout":null,"metadata":null,"text":"As previously discussed, we have many possible approaches for this problem. In this tutorial I’ll take standard deep learning detection approach, and will use the SSD detection model. We will use the SSD keras implementation from here. This is a nice Implementation by PierreLuigi. Although it has less GitHub stars than the rykov8 implementation, it seems more updated, and is easier to integrate. This is a very important thing to notice when you choose which project are you going to use. Other good choices will be the YOLO model, and the Mask RCNN.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_138.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_138.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_138.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_138.markups.0":{"type":"A","start":230,"end":234,"href":"https:\u002F\u002Fgithub.com\u002Fpierluigiferrari\u002Fssd_keras","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_138.markups.1":{"type":"A","start":325,"end":331,"href":"https:\u002F\u002Fgithub.com\u002Frykov8\u002Fssd_keras","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_138.markups.2":{"type":"STRONG","start":200,"end":209,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_139":{"id":"7ff608540e21_139","name":"bff4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4: Load and train the SSD model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_139.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_139.markups.0":{"type":"STRONG","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_140":{"id":"7ff608540e21_140","name":"5f0e","type":"P","href":null,"layout":null,"metadata":null,"text":"Some definitions","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_140.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_140.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_141":{"id":"7ff608540e21_141","name":"f701","type":"P","href":null,"layout":null,"metadata":null,"text":"To use the repo, you’ll need to verify you have the SSD_keras repo, and fill in the paths in the json_config.json file, to allow the notebook finding the paths.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_142":{"id":"7ff608540e21_142","name":"c4d0","type":"P","href":null,"layout":null,"metadata":null,"text":"Start with importing:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_143":{"id":"7ff608540e21_143","name":"a09a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import os\nimport sys\nimport skimage.io\nimport scipy\nimport json","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_143.markups.9","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_143.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.1":{"type":"STRONG","start":7,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.2":{"type":"STRONG","start":10,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.3":{"type":"STRONG","start":17,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.4":{"type":"STRONG","start":21,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.5":{"type":"STRONG","start":28,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.6":{"type":"STRONG","start":39,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.7":{"type":"STRONG","start":46,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.8":{"type":"STRONG","start":52,"end":58,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_143.markups.9":{"type":"STRONG","start":59,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_144":{"id":"7ff608540e21_144","name":"e787","type":"PRE","href":null,"layout":null,"metadata":null,"text":"with open('json_config.json') as f:     json_conf = json.load(f)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_144.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_144.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_144.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_144.markups.1":{"type":"STRONG","start":30,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145":{"id":"7ff608540e21_145","name":"6388","type":"PRE","href":null,"layout":null,"metadata":null,"text":"ROOT_DIR = os.path.abspath(json_conf['ssd_folder']) # add here mask RCNN path\nsys.path.append(ROOT_DIR)\n\nimport cv2\nfrom utils_ssd import *\nimport pandas as pd\nfrom PIL import Image\n\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline\n%load_ext autoreload\n% autoreload 2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_145.markups.16","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_145.markups.0":{"type":"STRONG","start":105,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.1":{"type":"STRONG","start":112,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.2":{"type":"STRONG","start":116,"end":120,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.3":{"type":"STRONG","start":121,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.4":{"type":"STRONG","start":131,"end":137,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.5":{"type":"STRONG","start":140,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.6":{"type":"STRONG","start":147,"end":153,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.7":{"type":"STRONG","start":154,"end":156,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.8":{"type":"STRONG","start":157,"end":159,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.9":{"type":"STRONG","start":160,"end":164,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.10":{"type":"STRONG","start":165,"end":168,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.11":{"type":"STRONG","start":169,"end":175,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.12":{"type":"STRONG","start":183,"end":187,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.13":{"type":"STRONG","start":188,"end":198,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.14":{"type":"STRONG","start":199,"end":205,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.15":{"type":"STRONG","start":213,"end":215,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_145.markups.16":{"type":"EM","start":52,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_146":{"id":"7ff608540e21_146","name":"adab","type":"P","href":null,"layout":null,"metadata":null,"text":"and some more definitions:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_147":{"id":"7ff608540e21_147","name":"b1f5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"task = 'svhn'","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_148":{"id":"7ff608540e21_148","name":"7a8d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"labels_path = f'{data_folder}pascal.csv'","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_149":{"id":"7ff608540e21_149","name":"b145","type":"PRE","href":null,"layout":null,"metadata":null,"text":"input_format = ['class_id','image_name','xmax','xmin','ymax','ymin' ]\n    \ndf = pd.read_csv(labels_path)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_150":{"id":"7ff608540e21_150","name":"e458","type":"P","href":null,"layout":null,"metadata":null,"text":"Model configurations:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_150.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_150.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_151":{"id":"7ff608540e21_151","name":"dd10","type":"PRE","href":null,"layout":null,"metadata":null,"text":"class SVHN_Config(Config):\n    batch_size = 8\n    \n    dataset_folder = data_folder\n    task = task\n    \n    labels_path = labels_path\n\n    input_format = input_format\n\nconf=SVHN_Config()\n\nresize = Resize(height=conf.img_height, width=conf.img_width)\ntrans = [resize]","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_151.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_151.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_151.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_151.markups.1":{"type":"STRONG","start":6,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_152":{"id":"7ff608540e21_152","name":"053f","type":"P","href":null,"layout":null,"metadata":null,"text":"Define the model, load weights","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_152.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_152.markups.0":{"type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_153":{"id":"7ff608540e21_153","name":"bbd9","type":"P","href":null,"layout":null,"metadata":null,"text":"As in most of deep learning cases, we won’t start training from scratch, but we’ll load pre-trained weights. In this case, we’ll load the weights of SSD model, trained on COCO data-set, which has 80 classes. Clearly our task has only 10 classes, therefore we will reconstruct the top layer to have the right number of outputs, after our loading the weights. We do it in the init_weights function. A side note: the right number of outputs in this case is 44: 4 for each class (bounding box coordinates) and another 4 for the background\u002Fnone class.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_153.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_153.markups.0":{"type":"A","start":138,"end":145,"href":"https:\u002F\u002Fdrive.google.com\u002Fopen?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_154":{"id":"7ff608540e21_154","name":"d011","type":"PRE","href":null,"layout":null,"metadata":null,"text":"learner = SSD_finetune(conf)\nlearner.get_data(create_subset=True)\n\nweights_destination_path=learner.init_weights()\n\nlearner.get_model(mode='training', weights_path = weights_destination_path)\nmodel = learner.model\nlearner.get_input_encoder()\nssd_input_encoder = learner.ssd_input_encoder\n\n# Training schedule definitions\nadam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) \nssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\nmodel.compile(optimizer=adam, loss=ssd_loss.compute_loss)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_154.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_154.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_154.markups.0":{"type":"STRONG","start":60,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_154.markups.1":{"type":"EM","start":289,"end":320,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_155":{"id":"7ff608540e21_155","name":"44a9","type":"P","href":null,"layout":null,"metadata":null,"text":"Define data loaders","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_155.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_155.markups.0":{"type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_156":{"id":"7ff608540e21_156","name":"4956","type":"PRE","href":null,"layout":null,"metadata":null,"text":"train_annotation_file=f'{conf.dataset_folder}train_pascal.csv'\nval_annotation_file=f'{conf.dataset_folder}val_pascal.csv'\nsubset_annotation_file=f'{conf.dataset_folder}small_pascal.csv'","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_156.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_156.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_156.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_156.markups.0":{"type":"STRONG","start":24,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_156.markups.1":{"type":"STRONG","start":85,"end":106,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_156.markups.2":{"type":"STRONG","start":147,"end":168,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_157":{"id":"7ff608540e21_157","name":"3ab2","type":"PRE","href":null,"layout":null,"metadata":null,"text":"batch_size=4\nret_5_elements={'original_images','processed_images','processed_labels','filenames','inverse_transform'}","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_158":{"id":"7ff608540e21_158","name":"adf7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"train_generator = learner.get_generator(batch_size, trans=trans, anot_file=train_annotation_file,\n                  encoder=ssd_input_encoder)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_159":{"id":"7ff608540e21_159","name":"14f2","type":"PRE","href":null,"layout":null,"metadata":null,"text":"val_generator = learner.get_generator(batch_size,trans=trans, anot_file=val_annotation_file,\n                 returns={'processed_images','encoded_labels'}, encoder=ssd_input_encoder,val=True)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_160":{"id":"7ff608540e21_160","name":"5f42","type":"H4","href":null,"layout":null,"metadata":null,"text":"5. Training the model","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_161":{"id":"7ff608540e21_161","name":"774e","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that the model is ready, we’ll set some last training related definitions, and start training","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_161.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_161.markups.0":{"type":"STRONG","start":0,"end":3,"href":"","anchorType":"LINK","userId":"","linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_162":{"id":"7ff608540e21_162","name":"babb","type":"PRE","href":null,"layout":null,"metadata":null,"text":"learner.init_training()","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_163":{"id":"7ff608540e21_163","name":"76e0","type":"PRE","href":null,"layout":null,"metadata":null,"text":"history = learner.train(train_generator, val_generator, steps=100,epochs=80)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_164":{"id":"7ff608540e21_164","name":"fd5a","type":"P","href":null,"layout":null,"metadata":null,"text":"As a bonus, I’ve included the training_plot callback in the training script to visualize a random image after every epoch. For example, here is a snapshot of predictions after sixth epoch:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_164.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_164.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_164.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_164.markups.0":{"type":"STRONG","start":30,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_164.markups.1":{"type":"STRONG","start":176,"end":181,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_164.markups.2":{"type":"EM","start":30,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_165":{"id":"7ff608540e21_165","name":"a054","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*24S3wR8AoFoes6Ddmn3rLw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*24S3wR8AoFoes6Ddmn3rLw.png":{"id":"1*24S3wR8AoFoes6Ddmn3rLw.png","originalHeight":283,"originalWidth":283,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_166":{"id":"7ff608540e21_166","name":"5f7c","type":"P","href":null,"layout":null,"metadata":null,"text":"The SSD_Keras repo handles saving the model after almost each epoch , so you can load later the models simply by changing the weights_destination_path line to equal the path","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:7ff608540e21_166.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_166.markups.0":{"type":"EM","start":126,"end":150,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:7ff608540e21_167":{"id":"7ff608540e21_167","name":"3c8b","type":"PRE","href":null,"layout":null,"metadata":null,"text":"weights_destination_path = \u003Cpath\u003E","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_168":{"id":"7ff608540e21_168","name":"6a7c","type":"P","href":null,"layout":null,"metadata":null,"text":"If you followed my instructions, you should be able to train the model. The ssd_keras provides some more features, e.g data augmentations, different loaders, and evaluator. I’ve reached \u003E80 mAP after short training.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_169":{"id":"7ff608540e21_169","name":"c821","type":"P","href":null,"layout":null,"metadata":null,"text":"How high did you achieve?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_170":{"id":"7ff608540e21_170","name":"154d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*y84JlUC2sOzql3um","typename":"ImageMetadata"},"text":"Training for 4X100X60 samples, from tensorboard","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*y84JlUC2sOzql3um":{"id":"0*y84JlUC2sOzql3um","originalHeight":242,"originalWidth":961,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:7ff608540e21_171":{"id":"7ff608540e21_171","name":"30b2","type":"H3","href":null,"layout":null,"metadata":null,"text":"Summary","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_172":{"id":"7ff608540e21_172","name":"5945","type":"P","href":null,"layout":null,"metadata":null,"text":"In this post, we discussed different challenges and approaches in the OCR field. As many problems in deep learning\u002Fcomputer vision, it has much more to it than seems at first. We have seen many sub tasks of it, and some different approaches to solve it, neither currently serves as a silver bullet. From the other hand, we’ve seen it is not very hard to reach preliminary results, without too much of hassle.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:7ff608540e21_173":{"id":"7ff608540e21_173","name":"214e","type":"P","href":null,"layout":null,"metadata":null,"text":"Hope you’ve enjoyed!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:ocr":{"id":"ocr","displayTitle":"Ocr","__typename":"Tag"},"Tag:computer-vision":{"id":"computer-vision","displayTitle":"Computer Vision","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"$Post:ee1469a201aa.previewContent":{"subtitle":"How and why to apply deep learning to Optical Character Recognition","__typename":"PreviewContent"},"$ROOT_QUERY.notificationsConnection({})":{"notifications":[],"pagingInfo":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).pagingInfo","typename":"Paging"},"__typename":"NotificationsConnection"},"$ROOT_QUERY.notificationsConnection({}).pagingInfo":{"next":null,"__typename":"Paging"},"$ROOT_QUERY.notificationStatus":{"unreadNotificationCount":0,"__typename":"NotificationStatus"}}</script><script src="./A gentle introduction to OCR - Towards Data Science_files/manifest.1aded676.js"></script><script src="./A gentle introduction to OCR - Towards Data Science_files/vendors_main.26101d5d.chunk.js"></script><script src="./A gentle introduction to OCR - Towards Data Science_files/main.2360d1c8.chunk.js"></script><script src="./A gentle introduction to OCR - Towards Data Science_files/vendors_screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_sc_873108c0.40da1149.chunk.js"></script>
<script src="./A gentle introduction to OCR - Towards Data Science_files/screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_screen.lan_edf4702d.3fed3768.chunk.js"></script>
<script src="./A gentle introduction to OCR - Towards Data Science_files/screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_screen.lan_624b8ed1.0d61fa66.chunk.js"></script>
<script src="./A gentle introduction to OCR - Towards Data Science_files/screen.post_screen.post.amp_screen.profile_screen.sequence.library_screen.sequence.post_screen.stori_aba94ffa.c6de8802.chunk.js"></script>
<script src="./A gentle introduction to OCR - Towards Data Science_files/screen.post.8a8b1dd3.chunk.js"></script><script>window.main();</script><script src="./A gentle introduction to OCR - Towards Data Science_files/p.js" async="" id="parsely-cf"></script></body></html>